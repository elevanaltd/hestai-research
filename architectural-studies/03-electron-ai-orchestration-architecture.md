Daedalus: Cross-Platform Electron AI Orchestration App Architecture Guide

Daedalus is a production-grade, cross-platform desktop application built on Electron. It orchestrates multiple AI large language model (LLM) providers with a unified interface, focusing on security, cost control, and performance. This guide outlines the architecture and implementation patterns for Daedalus, including the Electron process model, database integration, keychain usage, provider abstraction with fallbacks, state management, cost monitoring, security hardening, performance optimizations, error recovery, build/distribution, and testing/monitoring strategies. Code snippets and real-world references are provided throughout for clarity.

1. Electron Three-Process Architecture

Process Separation: Daedalus follows Electron’s multi-process model of one Main process and one or more Renderer processes, plus a Utility/Worker process for heavy background tasks. The Main process runs in a Node.js environment and is responsible for application lifecycle, creating windows, and orchestrating backend logic ￼. Each app window is a Renderer process running the React frontend (in Chromium), isolated from Node for security. This separation prevents a crash or hang in one renderer from taking down the entire app ￼ ￼. In addition, Daedalus spawns a background utility process to handle intensive operations (like LLM API calls and database work) off the main thread. This utility can be a Node.js worker thread or a forked child process dedicated to AI orchestration and database management. Keeping heavy CPU or I/O work out of the main process ensures the UI stays responsive ￼ ￼. For example, long-running token generation or vector searches run in a worker, and the main thread remains free to handle user input and window events.

Secure IPC and Sandboxing: Communication between processes is done via Inter-Process Communication (IPC) channels, with context isolation and sandboxing enabled for security. Renderer processes run with nodeIntegration: false and are sandboxed so they cannot directly use Node APIs or access the OS ￼ ￼. A preload script uses Electron’s contextBridge to expose a safe API for the renderer to send/receive messages to the main process without exposing sensitive capabilities ￼ ￼. All IPC messages are sent on predefined channels with structured, typed payloads (using TypeScript interfaces) to avoid mistakes. For example, the renderer might invoke window.electronAPI.sendRequest(data) which under the hood does ipcRenderer.invoke('llm-request', data), and the main process has an ipcMain.handle('llm-request', ...) that processes the request and returns a result. This request/response IPC pattern (using invoke/handle) keeps calls asynchronous and non-blocking, preventing the UI from freezing ￼ ￼. Long-lived streams use event-based IPC: the main process sends incremental updates via webContents.send('llm-stream', chunk) and the renderer listens via ipcRenderer.on('llm-stream', ...) to update the UI progressively ￼ ￼. All IPC handlers in the main perform input validation – e.g. if a renderer sends a file path to open, the main will check it’s within allowed directories – to guard against malicious or unintended messages.

WebSocket Streaming Integration: For real-time token streams from the AI providers, Daedalus uses a WebSocket-based channel between the main (or utility) and renderer. The main process runs a lightweight WebSocket server (using a library like ws) bound to 127.0.0.1 on a random port. When a new LLM request begins, the renderer opens a WebSocket connection to that server (e.g. ws://localhost:PORT) to receive the streaming tokens. As the utility process fetches streaming responses from the LLM API, it forwards each token over the WebSocket to the renderer in real time. This approach provides an efficient, push-based stream without the overhead of frequent IPC calls, effectively establishing a continuous duplex channel between main and UI ￼ ￼. The WebSocket messages carry data like {token: "...", done: false} which the renderer’s WebSocket client handles to update the chat view incrementally. Electron’s sandbox still applies (the renderer uses the standard browser WebSocket API), and the Content Security Policy is configured to allow connections to ws://127.0.0.1 only. In practice, this design enables smooth, low-latency streaming of LLM outputs – multiple tokens per second – without UI jank. (An alternative could be using Electron’s MessagePorts or the built-in IPC, but WebSockets simplify backpressure and streaming control). Once the stream ends (or if cancelled), the WebSocket is closed. This method has been used successfully in other Electron apps to create real-time streams between main and renderers ￼.

Debugging and Monitoring Multi-Process Flows: During development, Daedalus employs tools to inspect each process. The Renderer can be debugged via Chrome DevTools (opened with win.webContents.openDevTools()), and the Main process can be debugged by launching Electron with the --inspect flag or by using console logging. All IPC messages are logged (with channel names and payload summaries) to help trace interactions. We also integrate electron-devtools-installer to ease debugging. In production, for monitoring, we attach global error handlers: process.on('uncaughtException') in the main process to catch any unhandled errors (logging them and attempting graceful shutdown), and window.onerror in renderers to log errors from the UI code. Additionally, Electron’s crashReporter is enabled so that if a renderer or the main process crashes unexpectedly, a minidump is generated and uploaded to our error server for analysis ￼. The app listens for Electron’s render-process-gone event to detect renderer crashes and can respond by restarting the renderer (e.g. recreating the BrowserWindow or prompting the user to reload) ￼. In early beta, the team decided to auto-reload the window if a renderer crashed, so the app could continue running with minimal disruption ￼. In parallel, the crashReporter ensures we still collect the crash details for debugging later. For the background utility process, we monitor its health via heartbeat pings over IPC – if the main doesn’t receive a ping for some interval or if the child process exits unexpectedly, the main process will attempt to restart that worker and log the incident. This design – combined with extensive logging in each process – makes it easier to trace multi-process interactions and recover from crashes in one part of the app without losing the entire application.

2. Embedded Database Strategy

Daedalus uses two embedded databases: PostgreSQL for relational data and LanceDB for vector embeddings. Both run locally within the app’s install, enabling offline operation. This section covers how we bundle and manage an embedded Postgres server, securely store credentials, handle data files, and integrate LanceDB for AI vector search.

Embedded PostgreSQL Integration: Instead of requiring a separate DB install, Daedalus packages PostgreSQL and runs it within the app process. We use the Node library embedded-postgres to programmatically spawn a Postgres server on a free port ￼ ￼. At app startup, the main process initializes the database if needed (running initdb to create a new data directory on first run) and then starts the Postgres server in the background. For example:

import EmbeddedPostgres from 'embedded-postgres';
const pg = new EmbeddedPostgres({
  databaseDir: path.join(app.getPath('userData'), 'pgdata'),  // data directory for DB
  port: 5432,                 // or dynamic port
  user: 'daedalus',           // create a role for the app
  password: '...generated...',// secure password stored externally
  persistent: true            // ensure data persists on restart
});
await pg.initialise();  // sets up the data directory if not exists
await pg.start();       // launches the postgres server

Once running, the Node pg client (node-postgres) is used to connect via pg.getPgClient() and run queries ￼. We generate a random strong password for the Postgres role on first setup and store it securely in the OS keychain (see Keychain Integration below), rather than hardcoding it. This ensures that even if the data files are accessed, the credentials are not exposed in plaintext. On app launch, Daedalus retrieves the DB password from the keychain and uses it to start the embedded DB.

The PostgreSQL data directory lives under the user’s application data (e.g. %AppData%/Daedalus/pgdata on Windows or ~/Library/Application Support/Daedalus/pgdata on macOS). This keeps it separate from application binaries and allows users to back up or inspect the data easily. We manage the lifecycle of this data directory: if the user chooses to “Reset Application Data,” the app will shut down the DB and delete this folder (after confirmation). We also ensure the DB is cleanly shut down on app exit – the main process listens for the before-quit event and calls pg.stop() to gracefully stop PostgreSQL, flushing any unwritten data ￼.

Cross-Platform Support: The embedded-postgres library includes pre-built Postgres binaries for major OSes and CPU architectures. As of its current version, it supports Postgres versions 13-17 on Windows, macOS (x64/arm64), and Linux (x64/arm, etc.) ￼. We include the appropriate binaries during packaging so that at runtime, Daedalus can spawn the server regardless of platform. This was preferable to using SQLite because we needed advanced SQL features and concurrent connections that Postgres offers. (Note: Running a full Postgres inside an Electron app is uncommon and adds size overhead; we determined it was worth it for our needs. In general, if only a lightweight store is needed, SQLite is a simpler choice ￼, but Daedalus’s requirements justified Postgres).

Schema Management and Migrations: We treat the embedded Postgres like any server: the app on first run applies an initial SQL schema (creating tables for users, conversations, usage logs, etc.), and on upgrades, any schema migrations are run. We maintain SQL migration files and a schema version table. On startup, Daedalus checks the schema version in the DB and applies any pending migrations (within a transaction) before proceeding. This ensures forward-compatible upgrades. Because we bundle the DB, we control the version – if we ever need to upgrade the Postgres engine itself (say from 14 to 15), we can perform a dump-and-restore or use pg_upgrade behind the scenes. However, that adds complexity, so we try to stick with one major Postgres version to avoid user-impacting migrations.

Backup and Data Safety: For user peace of mind, Daedalus offers an option to export the database (via a simple UI action). This internally runs pg_dump (using the embedded server’s binaries) to produce a plain SQL backup of all user data, which the user can save. We advise users to do this before major updates. The app could also perform automatic mini-backups (e.g. dump critical tables to a JSON in userData) at intervals, but currently it’s on-demand. The embedded Postgres is configured with safe defaults (e.g. using fsync to ensure data integrity on disk). For performance, we might tune down some settings given the local single-user usage – for example, we can allow more aggressive autovacuum or set a smaller shared buffer since typically only our app connects. We use the library’s options to pass custom flags; for instance, we set postgresFlags: ["-N", "100"] to limit max connections (since only a few connections from the app are needed) and initdbFlags: ["--locale=C"] to avoid locale overhead ￼. These little tunings keep the footprint low.

LanceDB for Vector Storage: LanceDB is an embedded vector database optimized for similarity search in local apps. It stores high-dimensional embeddings in an on-disk columnar format (Apache Arrow) that enables fast vector operations. We include LanceDB via its Node.js package (vectordb), which bundles native code for each platform (with .node binaries). LanceDB runs in-process – no separate server – making it straightforward to use alongside Postgres. On startup, we initialize a LanceDB database file (or directory) in the user data folder (e.g. vectors.lance in the same app data directory). The LanceDB API allows us to create tables for different embedding sets; for example, we might have a documents table with columns for id, text, and vector. To generate embeddings, Daedalus uses a pipeline: for instance, when a user adds a document or when new chat context is saved, the app calls an embedding model (could be OpenAI Embeddings or a local model) to compute the vector, then inserts the vector into LanceDB along with the reference to the original data. LanceDB is lightweight and on-premise, so all vector search queries are local and fast ￼ ￼ – no API calls required, and no data leaves the user’s machine.

We ensure consistency between Postgres and LanceDB by using a shared ID. For example, when a new knowledge document is added, we insert the document text and metadata into Postgres (getting an doc_id), and we insert the embedding into LanceDB with that same doc_id. If either insertion fails, we roll back the other so they stay in sync. This way, when the app performs a semantic search (vector similarity query) in LanceDB, the result includes a doc_id which we then use to look up the full document info from Postgres. A periodic job reconciles the two stores (e.g. if a LanceDB entry has no matching Postgres record or vice versa, it logs a warning or fixes it). In practice, this two-DB approach behaves like a hybrid database: Postgres is the system of record for structured data and LanceDB serves vector search for unstructured data.

Semantic Search Performance: LanceDB is designed for speed – it uses an Arrow-based format that can scan large vector sets quickly, and it may implement approximate nearest neighbor indexing under the hood. We found it performant for our use case (thousands to low millions of embeddings). Still, we take care to maximize performance: we batch insert vectors (inserting 100s at a time) to amortize overhead, and we use lazy loading of vectors when possible. For example, if the user’s query can be resolved via an index in Postgres (like exact keyword match), we don’t hit LanceDB. But when we do need semantic search, we execute a LanceDB similarity query (e.g. table.search(vector, k=5)) to get the top results. The LanceDB query returns in-memory objects that we can map to Postgres IDs and then retrieve those records. Because LanceDB is embedded, there’s no network latency – queries are just function calls in memory, and data is stored on disk in a format optimized for sequential reads. This yields excellent latency for vector searches, typically millisecond-level for a few thousand vectors. We also monitor LanceDB performance and size: as the vector store grows, we may introduce indexing (if LanceDB supports HNSW indexing or IVF, we would use those to keep search fast). If needed, we could periodically rebuild indexes or compress older vectors. LanceDB’s design as a local, serverless library makes it ideal to pair with Postgres for retrieval-augmented generation use cases ￼ ￼.

Secure Data and Secrets: The databases may contain sensitive information (conversation histories, API usage logs). All data at rest is stored on the user’s machine. We rely on OS-level disk encryption (if enabled) for general protection. For added security, we plan to offer an option to encrypt the Postgres data directory with a user-supplied passphrase (using Postgres’s encryption or via filesystem encryption tools) – this is an advanced feature beyond initial scope. Key secrets (like API keys used to call LLM providers) are never stored in Postgres; they are kept only in OS secure storage (or in memory when in use). Additionally, the LanceDB vectors, while not directly interpretable, could be considered sensitive, so similarly they reside only locally. If the user chooses to wipe data, our app will delete the Postgres data directory and LanceDB files securely.

Data Directory Maintenance: Over time, the Postgres data directory can grow (especially if usage logs accumulate). We implement a few maintenance tasks: the app runs VACUUM on Postgres periodically (perhaps during idle times) to reclaim space from deleted rows. We also set up autovacuum settings on the embedded server tuned for a desktop app (e.g. more frequent but less intensive runs given the smaller scale of data). For LanceDB, if we notice the file size growing significantly, we may schedule a rebuild/compaction (if LanceDB provides such an API to garbage-collect deleted vectors). Backups were discussed above; restoring from backup would involve stopping the embedded DB, replacing files or running psql to restore, etc., which we document for advanced users.

In summary, the embedded database strategy gives Daedalus full control over data locally, with Postgres providing robust relational storage and LanceDB enabling high-performance vector similarity search – all bundled seamlessly in an Electron app.

3. OS Keychain Integration (Secure Credential Storage)

Security is paramount in Daedalus, so we never store API keys or passwords in plain text. Instead, we leverage each operating system’s secure credential storage: Keychain on macOS, Windows Credential Manager, and Linux Secret Service (Libsecret/Gnome Keyring). The cross-platform solution we use is the Node package keytar, which provides a unified API to these OS keychains ￼. When the user enters an API key (for OpenAI, Anthropic, etc.) or any other secret, the app calls keytar.addPassword(service, account, secret) to save it securely. For example:
	•	Service is set to the application name (e.g. “Daedalus”).
	•	Account is a descriptor of the secret, e.g. “OpenAI API Key”.
	•	Password is the sensitive value (the API key string).

Keytar will store this credential in the OS-provided vault – on Mac, it creates an entry in the user’s login Keychain; on Windows, in the Credential Manager; on Linux, in the Secret Service (which usually requires the user to be logged in to a session). When the app needs the key (for example, to make an API call), it calls keytar.getPassword(service, account) to retrieve it. The OS may prompt the user the first time for permission (especially on Mac, a dialog might appear asking if the app can access the keychain item). Once allowed, subsequent accesses are silent ￼. This approach ensures the keys never touch disk in plaintext and are protected by OS-level encryption tied to the user’s login ￼ ￼. Even if an attacker obtained the app’s files, they wouldn’t find the API keys there.

To implement this, Daedalus prompts users on first launch (or first use of a given AI provider) to enter their API key. We explain it will be stored securely. The main process uses keytar to save it. The key is kept out of renderer processes entirely – the renderer might invoke an IPC call like saveApiKey("openai", key), and the main handles that by calling keytar. This way, the sensitive string only exists in the main process memory briefly and then inside the OS vault. When making requests, the main retrieves the key and uses it; the key is never exposed to the React front-end or any web context, fulfilling the requirement that API keys never touch the renderer.

Cross-Platform Fallbacks: In most cases, keytar works out-of-the-box. On Windows and Mac it should always succeed (assuming the user account is accessible). On Linux, there’s a dependency on libsecret/Secret Service – if the user’s environment doesn’t have a keyring (for instance on a headless Linux or a minimal distro), keytar.addPassword might fail. We handle this scenario by detecting keytar errors and falling back to an alternative. One fallback strategy is to use Electron’s safeStorage module. Starting in Electron 15, safeStorage can encrypt and decrypt strings using the OS facilities (DPAPI on Windows, Keychain on Mac) without an external dependency ￼. If keytar is unavailable, we do the following: use safeStorage.encryptString(apiKey) to get an encrypted buffer, then store that buffer (as binary or Base64) in an application config file. Later we read it and decrypt via safeStorage.decryptString(buffer). The encryption key is managed by the OS (tied to the user session), so this is still secure. This approach was used successfully by other apps to replace keytar and simplify builds ￼ ￼. The only downside is that the encrypted data is only usable on the same machine/user. In Daedalus’s context, that’s fine (keys are not meant to roam).

Another fallback, if even safeStorage is not available (Electron provides safeStorage.isEncryptionAvailable() to check), is to prompt the user each time for the key rather than storing it. This is worst-case, but it may be necessary in a scenario like a very minimal Linux without any secure storage – we prefer inconvenience over insecure storage. The app will notify the user that it couldn’t store the credential securely and that they’ll need to re-enter it each run (fortunately rare).

Usage and Rotation: With keys in the keychain, whenever we need to call an API, the main process fetches the latest key value. If the user wants to update their API key (for example, if they regenerate it on OpenAI’s dashboard), they can do so in the app’s settings. We call keytar.replacePassword(service, account, newKey) – this updates the entry in-place ￼. If the user deletes a key (maybe revoking access), we call keytar.deletePassword(service, account) to remove it from the vault. We also handle the case where the user manually deletes the stored key outside the app (e.g. via Keychain Access on Mac); in that event, keytar.getPassword will return null, and our app will prompt for the key again ￼. We thus always check the return and never assume the key is present.

Security Benefits: Using the OS keychain means the OS isolates our secrets from other apps. On Mac, by default only our application (identified by bundle ID) can access the entry without prompting ￼. On Windows, the credential is stored under the current user’s context. This is far more secure than, say, writing to an .env file or storing keys in the app’s database – even if an attacker found a way to read those, they’d just find an encrypted blob or nothing at all. A Reddit discussion on Electron best practices confirms that letting the user input the key and storing it in keytar is the recommended approach (to avoid packing any secret with the app) ￼.

In summary, Daedalus never hard-codes secrets or writes them unencrypted. We rely on the proven native keychain solutions via keytar (and Electron safeStorage as needed) to keep API tokens and passwords safe. This strategy aligns with Electron security guidelines and ensures that if someone were to inspect the app’s files, they would not find any usable credentials.

Implementation note: Keytar is a native Node module, so we include precompiled binaries for it or compile it during install. We ensure our build pipeline or installer runs electron-rebuild so that keytar’s native addon is compatible with our Electron version. This is tested on all platforms. In case of build complexities, safeStorage is an attractive alternative since it doesn’t require an extra native module – in fact, one of our inspirations, the Ray app, switched from keytar to safeStorage to simplify multi-platform builds ￼ ￼. We may consider a similar switch in the future if it further improves reliability.

4. AI Provider Abstraction and Fallback Orchestration

Daedalus is designed to work with multiple AI providers (OpenAI GPT-4/4.1, Anthropic Claude v4, Google Gemini 2.5, etc.), yet present a unified chat experience. This is achieved by an abstraction layer for LLM providers and a fallback mechanism to seamlessly switch models/providers when needed. Key features of our design include standardized interfaces, streaming support, usage tracking per provider, and robust error handling with graceful fallback to secondary providers.

Abstracted Provider Interface: We define a common TypeScript interface (or abstract class) for an LLM Provider, something like:

interface AIProvider {
  name: string;
  supportsStreaming: boolean;
  maxTokens: number;
  call(prompt: string, options: CallOptions): Promise<AIResponse>;
  stream(prompt: string, options: CallOptions): EventEmitter; // emits 'data' events for tokens
  // ... possibly methods for token counting, etc.
}

Concrete classes implement this for each provider’s API. For example, OpenAIProvider uses OpenAI’s REST API, AnthropicProvider uses Anthropic’s API, etc. Each class encapsulates the specifics of authentication (e.g. API keys and endpoints) and request/response format. This layer makes the rest of the app provider-agnostic: the orchestration code can call provider.call() without knowing if it’s talking to OpenAI or Google – this eliminates duplicate code and makes adding new providers easier in the future.

Streaming Differences: While all target providers offer streaming, the details vary. OpenAI’s ChatCompletion streaming returns incremental JSON chunks with a delta field containing token text; Anthropic’s streaming returns textual SSE (server-sent events) with parts of the completion; Google’s might have its own structure. Our abstraction normalizes these into a single stream interface for the app. For instance, our AIProvider.stream() returns an EventEmitter or Readable Stream where each data event carries a simple {token: string} payload. Under the hood, the OpenAI implementation will listen to the SSE data events (which might look like {"choices":[{"delta":{"content":"hello"}}]}), extract the content ("hello"), and emit {token: "hello"} to the app. Anthropic’s stream might already deliver plain text chunks, so it can emit them directly. By abstracting this, the renderer doesn’t need separate logic per provider – it just uses the unified WebSocket/IPC streaming channel to get token events from whichever provider is currently active.

We also handle stream completion signals. For example, OpenAI sends a "role": "assistant" and empty content to signal the end; Anthropic might send a stop reason. The provider implementation catches these and will emit a special {done: true} event or simply end the stream so that the app knows the response is complete. All providers have subtle differences (e.g. maximum token limits, required parameters like system prompts, etc.), which the abstraction layer manages internally.

Single Active Provider & Fallback Chain: At any given time, only one provider is generating a response (to avoid interleaving outputs). However, Daedalus implements an automatic fallback chain: if a request to the primary provider fails (e.g. due to outage or rate-limit), it will automatically try the next provider in priority order. The user can configure the priority (say OpenAI GPT-4 first, then Claude second, then Gemini third). The orchestration logic does something like:

for (const provider of providersInPriority) {
  try {
    const stream = await provider.stream(prompt, opts);
    activeProviderName = provider.name;
    // pipe stream to UI...
    await handleStreamToCompletion(stream);
    break; // success, exit loop
  } catch (err) {
    log(`${provider.name} failed: ${err.message}`);
    if (isRetryableError(err) && provider !== lastProvider) {
      continue; // try next provider
    } else {
      throw err; // non-retryable or no fallback left
    }
  }
}

This ensures high availability: if GPT-4 is down or returns an error for a given query, the user doesn’t get a failure – instead, Claude is engaged automatically to attempt the response ￼ ￼. We inform the user in such cases (e.g. a message “⚠️ OpenAI failed, switching to Claude for this query” could be shown in the chat). The switching happens transparently mid-stream if needed. For example, if the primary provider fails immediately (before streaming any tokens), we start streaming from the fallback after a brief delay. If a failure happens partway (less common, but say the stream cuts out due to network), we have to either retry the same provider or start over with a fallback model; our strategy is usually to abort and retry the whole request with the next provider, as partial answers would differ.

We implement retry logic with backoff for transient errors. For instance, if OpenAI times out or returns a 500 error, we might retry OpenAI once or twice before falling back, depending on error type. We use an exponential backoff delay (e.g. wait 1s, then 2s) to give the provider a chance to recover ￼ ￼. Only certain errors trigger fallback – e.g. network errors, overload, 5xx responses, etc. If the error is a user issue (like invalid API key or content filter rejection), we won’t keep falling back pointlessly; in such cases we surface the error to user instead. The Helicone project demonstrates a similar idea of specifying fallback providers in a prioritized list that automatically get tried on failure ￼ ￼.

This robust error handling and fallback mechanism greatly improves reliability ￼. In production AI systems, you don’t want a single provider outage to break your app ￼ ￼. For Daedalus, if the user has keys for multiple providers configured, the app will silently ensure an answer is returned as long as at least one provider works. And if all providers fail (rare), then after exhausting the list we return a graceful error message to the chat like “All providers are currently unavailable. Please try again later.” ￼. This is better than a crash or an endless hang.

Rate Limiting and Throttling: Each provider has its own rate limits (requests per minute, tokens per minute, etc.). Our abstraction layer can enforce these to avoid hitting hard errors. For example, OpenAI might allow 3 requests/minute on a certain key – we maintain a rolling counter and if the user triggers requests too fast, we queue or delay them with a friendly “Rate limit reached, waiting 20s” message. This is done in the main process orchestrator. Similarly, if a provider has a max tokens per minute limit, we track how many tokens we’ve sent and received in the current window. Since Daedalus also tracks cost, it already counts tokens (see Cost Control section), so it can reuse that info for rate limiting. The app will prioritize not going over known limits by either spacing out calls or switching provider if one is temporarily saturated (for example, “OpenAI quota exhausted, falling back to Claude”).

Token Counting and Cost Tracking: The provider interface includes methods or data for token usage. OpenAI responses include a usage field with prompt and completion token counts ￼, which we capture. Anthropic doesn’t provide an explicit count in the API response ￼, so we estimate by counting tokens in the prompt and response using the model’s tokenizer. We have integrated OpenAI’s tiktoken tokenizer (via an unofficial Node port or WASM) to count tokens for models using similar encoding, and for Anthropic we use a comparable method (Anthropic uses a GPT-2 style tokenizer). Google’s API might provide token counts in response (not certain until we see their format), but if not, we handle it similarly. These counts are crucial for cost tracking: each model has a known price per 1K tokens (for instance, GPT-4 is $0.03/1K prompt tokens and $0.06/1K output tokens as of this writing). We maintain a table of pricing for each model (which we can update as providers change prices). After each call, we calculate the cost: e.g. if 500 prompt + 1000 completion tokens used on GPT-4, cost = 1500/1000 * $0.06 = $0.09 (assuming all were output tokens cost for simplicity) ￼. We also track ongoing usage: we log each call’s token count and cost in Postgres (with a timestamp and provider info). This feeds into the budget enforcement discussed later.

Provider-Specific Features: The abstraction tries to cover common functionality, but some providers have unique features (like OpenAI’s system and function messages, or Google’s groundedness settings). We expose a superset of features through the interface’s options. For example, CallOptions may include a “systemMessage” or “temperature” or “model” (for OpenAI you might select gpt-4 vs gpt-4-32k). Not all options apply to all providers (we document this in our code). If a certain feature is unsupported by the current provider, we either ignore it or simulate it. For instance, if OpenAI allows 4 roles in messages but another API expects just a single prompt string, our code will concatenate system+history+user messages into one prompt for that provider. This way, the app’s higher layer can use advanced prompting techniques without worrying about which API accepts what – the provider class handles it.

Cost and Preference Awareness: The orchestrator not only falls back on failure, but could also choose providers based on cost or performance. For example, if a user sets a cost-savings mode, the app might use a cheaper model (Claude or an OpenAI 3.5) by default and only use GPT-4 if explicitly needed. We allow such configurations. Providers have metadata like costPer1kTokens and maybe an estimated quality score, so the app can make informed decisions (e.g. use GPT-4 for critical queries, otherwise GPT-3.5 to save money). In fallback, we generally go from highest quality to next best – but if cost is a concern, a user might invert that order. The system is flexible to support that policy.

Streaming UX: Because only one provider streams at once, the user experience is simply that the message response starts appearing a few seconds after they submit, and it will come from whichever provider responded first successfully. If a fallback had to happen, there might be a slight additional delay, but we try to minimize it. For example, if OpenAI fails immediately with a 503, the fallback (Claude) might start within 1 second. We log which provider was used for each answer (and show an indicator in the UI, like a little logo or text, so the user knows “Answer generated by Claude” for instance). This increases transparency.

In summary, our multi-provider orchestration is built for resilience and abstraction. By decoupling provider specifics (via the interface layer) and implementing automatic retries/fallbacks ￼, Daedalus provides a smooth experience where model switching is invisible to the user except for a small notification. We treat LLMs as swappable back-end engines – much like a database driver – to avoid vendor lock-in and maximize uptime ￼.

5. State Management Across Processes

Daedalus uses React + Zustand for front-end state management, and synchronizes application state between the Renderer and Main processes via IPC. The challenge is that some state (e.g. conversation data, provider status, usage stats) is updated in the background (main process or utility) and needs to reflect in the UI, and vice versa (user actions in UI need to update state in main). We implement a single source of truth pattern, with the main process holding the authoritative state and renderer stores mirroring it.

Zustand in Renderer: Zustand is a lightweight state management library that works in React apps. In the renderer, we create Zustand stores for UI state (such as the current conversation, settings, etc.). Normally Zustand would just manage state within the renderer. However, to keep state in sync with the main process, we integrate an IPC sync mechanism. We considered using a community solution like Zutron/Zubridge, which enables a single Zustand store across Electron processes ￼ ￼. This works by creating a master store in the main process and replicating its state to renderer stores, dispatching actions via IPC to the main, and broadcasting updates back. In fact, Zutron’s approach is to intercept store updates in the renderer and forward them to the main store, then push the new state out to all renderers ￼. Using such a library or pattern, we achieve a unified app state accessible in all contexts. This prevents issues where each window or process has divergent state.

In practice, our implementation is as follows: the main process hosts a central store (which can be a vanilla Zustand store without React). This store holds key slices of state such as: user profile, active conversation and messages, list of available providers and their status (e.g. API key loaded or not), usage stats, etc. The renderer has its own Zustand store, but it doesn’t independently modify the critical pieces of state – instead, it acts as a cache of the main state. We set up IPC channels for state update events. Whenever the main state changes, the main process sends an IPC message like state-updated with the changed slice or a diff. The renderer’s preload script listens and applies those changes to the local Zustand store. Conversely, when the user triggers an action that changes state (e.g. toggling a setting), the renderer doesn’t finalize it locally; it sends an IPC call to the main (like ipcRenderer.send('update-setting', key, value)). The main process then updates its store and broadcasts the change. The renderer’s store update comes via the broadcast event, at which point the UI re-renders with the new value. This ensures consistency and that only the main process mutates the true state.

We do allow some optimistic updates in the UI for better UX. For instance, when the user sends a new chat message, we immediately show it in the UI (adding to the conversation state) before it’s confirmed saved in the database or before the AI response arrives. The renderer will add a pending message to its Zustand store locally (so the message appears right away). Simultaneously, it notifies the main process of the new user message. The main process will officially add the message to the conversation state (persisting to Postgres) and send back a confirmation or any adjustments. If something went wrong (perhaps the DB is full or some error), the main would send back an error and the UI can then remove the optimistic message or mark it with an error state. Typically though, the optimistic update is confirmed and the state remains consistent. Another example: toggling a “dark mode” setting could instantly update UI theme (optimistic) while informing the main to save the preference. We implement these carefully to avoid conflicting updates.

State Reconciliation: Because of the asynchronous nature of IPC, it’s possible (though rare) that the renderer and main might have slight state mismatches (for example, if a renderer made multiple rapid updates that haven’t all applied in main yet). To mitigate issues, we sometimes perform full state syncs at certain points. For example, when a new window (renderer) opens, it requests the entire current state from main (ipcRenderer.invoke('get-full-state')). The main responds with a snapshot of state (maybe as a JSON). The renderer then hydrates its Zustand store with that. After that point, incremental updates suffice. We also do a sanity sync when coming back from a suspended state (e.g. if the app was minimized or computer slept, on resume we ask main for any state changes that might have occurred). This keeps everything in lock-step.

Streaming State Updates: Streaming data (like the incoming AI response tokens) is also reflected in state. The main process, as it receives tokens, will update the current message’s content (appending new tokens) in its state and emit an update. However, updating on every single token could be too frequent. Instead, the main batches these updates slightly – for instance, it might accumulate tokens for 50ms and then send one state update with the aggregated text. The renderer’s store update then appends that chunk. This way we reduce IPC chatter while still appearing real-time. If using WebSocket as described, the renderer might directly get token messages and update its state. In our architecture, we actually do both: the tokens are streamed via WebSocket for immediate UI effect (since that bypasses any overhead), and the main also builds the final message text and when the stream ends, the main sends a final state update ensuring the renderer has the exact full message. This double approach guarantees no drift (in practice the renderer was already showing it, but the final sync from main is a confirmation/fix in case of any missed packet).

Zustand Sync via IPC: We drew inspiration from approaches shared by the community. One Reddit user described using Zustand’s setState inside IPC listeners to sync state updates from main to renderer ￼. Essentially, our preload registers IPC handlers that call the renderer store’s setState with the payload from main. Conversely, when renderer wants to update, we can either dispatch an action to main or use a middleware that intercepts store updates and sends them to main. Libraries like Zutron (now Zubridge) do exactly this: “Zutron enables a single store workflow across the IPC boundary, working with Electron security recommendations” ￼ ￼. We opted for a custom implementation tailored to our state shape, but the concept is the same. The main advantage is state persistence and logic live in one place (main). The renderer stores are essentially read-only mirrors (except for the optimistic UI-only fields). This prevents issues of having to merge divergent states or pass too many messages. It’s akin to a client-server model but both client (renderer) and server (main) are on the same machine and communication is fast.

Persistence of State: Some parts of state should persist between app runs, e.g. user settings, last open project, etc. For those, we use a combination of databases and config files. User preferences (theme, UI layout) are stored using a lightweight JSON config via electron-store, which the main process reads/writes ￼. This electron-store is exposed to the renderer through preload for convenience (read-only) but only main writes to it ￼ ￼. Conversation history and such are stored in Postgres – thus persisted on disk already. On startup, the main process loads necessary state (like user profile from DB, preferences from config) into the global state store. Zustand allows initial state hydration easily. Then new renderer windows get that state as mentioned.

Multi-Window and Future Multi-View Support: While the primary UI is one window, our approach would support multiple windows (each renderer getting the same state updates). For example, if in future Daedalus has a detached conversation viewer or a separate settings window, all renderers connect to the same main state. The main process broadcasts state changes to all renderer processes via their webContents.send. Zustand’s recommended single-store approach fits well: “use a single store in your app”, even in Electron ￼ ￼. Our synchronization ensures all windows reflect the same global state (except any window-specific UI state which they handle individually).

Example: Suppose the user’s budget usage reaches 50%. The main process updates the usage stat in its state and sends state-updated: {"usage": { ... }} to renderers. The Zustand store in the renderer picks this up and now the UI progress bar shows “50% of budget used”. Conversely, if the user toggles “Auto-switch model on fail” setting in the UI, the renderer sends an IPC like updateSetting('autoFallback', true). The main updates its config state, persists it to disk, and emits state-updated: {"settings": {"autoFallback": true}}. The renderer store updates, though in this simple case the renderer already optimistically set the toggle to true (so nothing visibly changes, but if another window was open it would now update too).

By centralizing logic in main and using a predictable message flow, we avoid complex multi-source synchronization issues. The pattern is essentially a unidirectional data flow: Renderer UI events -> main process actions -> main state change -> broadcasts -> renderer state update -> UI rerender. This is very much like a Redux architecture (main as store) but implemented with lightweight tools (Zustand + IPC) ￼ ￼.

In development, this approach proved to work well – we can even hot-reload renderer code and it reattaches to the current state from main (since main retains the state unless the whole app restarts). We also wrote tests for the state sync (ensuring that if two updates happen quickly, the final state is correct in both main and renderer, etc.). The result is that the user always sees a coherent state, no matter which process initiated a change.

6. Cost Control Architecture

Controlling and monitoring API usage cost is a key design aspect of Daedalus. We implemented a real-time token counting and budgeting system to ensure users don’t accidentally incur large bills. The system tracks tokens and dollars for each provider, enforces user-defined budgets, provides notifications, and can halt processes if limits are exceeded (the “emergency stop”).

Real-Time Token Counting: As mentioned earlier, every AI provider call yields token usage data which we capture ￼. For streaming calls, we don’t wait until the end – we count tokens on the fly. The provider abstraction emits tokens; for each token received, we increment a counter for that session. We know the prompt token count before streaming (we count those upfront when the request is made), and then count each output token as it streams. This allows us to display a running tally in the UI (for example, a small counter might say “Output tokens: 150” updating as the answer is generated). It also feeds into cost calculation continuously. We maintain a per-request token counter and also accumulate to a daily and monthly total.

Our approach for counting is to use the model’s known tokenizer. We integrated OpenAI’s tokenizer (tiktoken) in the main process to count the prompt tokens precisely – e.g. countTokens(promptString, modelName). For output, counting each token event is simpler (each event often is one token or part of a token). We also use heuristic fallback: for some providers where counting precisely might be complex, we approximate by character count (with a conversion ratio). This approximation is usually within a few percent of actual tokens, which is acceptable for live display (and then we correct with actual usage from the API response if provided at the end).

Dynamic Pricing and Model Costs: We maintain a JSON of model prices (cost per 1K tokens for prompts and completions, which differ for some providers). This table can be updated by us (through app updates or even remotely fetched if we choose). We periodically verify if providers changed their pricing (OpenAI sometimes announces new prices – we could push those updates). When calculating cost, we use the latest known price. For example, if GPT-4 output 1000 tokens, we multiply by $0.06 per 1K = $0.06. If the model was Claude 4, which might have a different pricing, we use that. We also handle multiple models within a provider – if the user switches to a different OpenAI model (say gpt-3.5) for some queries, the cost accounting uses that model’s rates.

We built a small feature to update pricing dynamically: the app can fetch an official pricing page or API (if available) occasionally and parse new rates. However, since pricing changes are infrequent and we want offline capability, the app also includes a default price list. The user can also manually edit the prices in an advanced settings panel (in case they have a custom rate, e.g. if using Azure OpenAI which might have different costs, they could adjust accordingly in the config).

Budget Enforcement: Daedalus allows the user to set a monthly budget limit (e.g. “Do not exceed $20/month on API calls”) and optionally per-day or per-session limits. The app tracks the cumulative cost in the current period. For instance, if the user sets $20/month, the app resets the count at the start of each month. The usage data is stored in Postgres – each API call log has a timestamp and cost. Summing those for the current month gives current spend. We cache this value in the state for quick checks and update it on each new call. If a new request would cause the budget to be exceeded, the app will either warn or block it depending on user preference. By default, we warn and ask confirmation: “This request may exceed your monthly budget of $20. Continue?” If the user confirms, we allow it (and maybe flag that they exceeded budget). If they decline, we cancel the request. There’s also an “auto-stop” setting where the user can choose to have the app hard-stop without asking once budget is hit, as a safety measure.

We also implement soft limits – e.g. a user can set a warning threshold at 90% of budget. When our tracked spend passes that threshold, we trigger a notification (a dialog or in-app banner) informing them “You have used 90% of your monthly AI budget.” This gives users awareness to avoid surprises. These thresholds and budgets are user-configurable.

User Notifications: All cost-related events are communicated to the user in a clear way. We integrated with the OS notification system for critical alerts (like if an “emergency stop” kicked in, we might fire a desktop notification saying “Daedalus has paused AI requests – budget limit reached”). Within the app UI, there’s a usage dashboard that updates in real-time: showing tokens used and cost for the current conversation, and overall usage this month. If a fallback to a more expensive model occurs, we might also notify: for example, if the app had to use GPT-4 (which is costly) because the cheaper model failed, and this significantly impacts cost, the user should know. A small inline message might say “Claude is not responding, switching to GPT-4 (note: higher cost per request)”.

We have an “Emergency Stop” feature: essentially a kill-switch the user can hit (or that triggers automatically if budget is exceeded by a certain margin) which immediately aborts all ongoing LLM requests and disables new ones until re-enabled. The user can think of it as a panic button if they see something has gone wrong (imagine a bug causing a loop of requests – costs could mount quickly; the user can slam the stop button to cut it off). Internally, this sets a flag that the orchestrator checks before starting any new request or continuing streams. If activated, it cancels streams (we close the WebSocket or set a flag that makes the token handler drop further tokens) and returns control to the UI. The UI then prominently shows “AI requests paused” until the user manually resumes and perhaps increases their budget or acknowledges the risk.

Cost Display and Transparency: Throughout the UX, we emphasize transparency of usage. Each chat response, when finished, shows a small subscript like “(Model: GPT-4, Tokens: 1,500, Cost: $0.09)” so the user sees what that answer cost ￼. The cumulative total is shown in the app header (especially if a budget is set, e.g. “This month: $5.40 of $20 used”). We found that making cost visible actually helps users tailor their usage (like they might decide to use a cheaper model for trivial questions or shorten their prompts). This aligns with best practices to prevent runaway costs.

Dynamic Adjustments: Our cost control architecture can also dynamically respond to conditions. For example, if the user is on a limited budget and one provider is significantly cheaper, the app can suggest “Consider switching to X model to save costs.” We haven’t fully automated this (we don’t auto-change the provider solely due to cost, because quality matters), but we do inform the user of cost differences. In the future, we might allow a mode where if budget is close to limit, automatically use the cheapest provider to stretch it.

Emergency Stop Implementation: Automatic triggers for the emergency stop could include: budget exceeded, a single request that would cost above a certain threshold (for example, if the user pastes a huge document and asks to summarize, the app can estimate the token count – if it’s extremely large and would cost say $2 for one response, the app can prompt confirmation first, or stop it if not allowed by user settings). Also, if the app detects an anomalous spike – e.g. normally calls are <1000 tokens, but suddenly a call is going into 10000+ tokens (maybe an LLM is rambling on), we might halt further generation and ask the user if they want to continue and incur the cost. The user could then explicitly continue (perhaps we’ll implement a “continue response” button that resumes streaming after such a pause). This is akin to how some apps ask “load more” for long content to avoid unbounded usage.

From a technical perspective, the main process’s orchestrator monitors the token count during streaming. If a threshold is hit, it can send a special message to renderer to pause output and ask for input. Because LLM APIs don’t have an out-of-the-box way to pause a stream (other than closing the connection), we would actually have to terminate the request. If the user then allows continuation, we might either resume by calling the API again with the remainder of the prompt (not always possible) or just let it go on from the last state if the API supports a resume token. This part can be tricky; as a simpler approach, we generally rely on max token limits to avoid infinite outputs, and set those max tokens according to user’s settings. E.g., if a user has only $0.50 left, we won’t allow a 5k token completion to be generated; we’d set a lower max token count or warn them.

In summary, our cost control architecture combines real-time monitoring of token usage with user-defined limits and fail-safes. It gives users insight and control over how much they spend on AI API calls, which is essential for a self-hosted orchestration app. Users can confidently use Daedalus knowing it will track every penny and protect them from surprises. This level of cost awareness and control is a distinguishing feature that we built into the core of the app logic.

7. Security Hardening

Security is built into every layer of Daedalus. We adhere to Electron’s security best practices ￼ and add our own measures specific to AI data handling. Below is a breakdown of key security features and a checklist of hardening techniques:

Content Security Policy (CSP): We define a strict CSP to prevent any unauthorized scripts or resource loads ￼. The app’s HTML includes a <meta http-equiv="Content-Security-Policy" content="default-src 'self'; connect-src 'self' https://api.openai.com https://api.anthropic.com ws://127.0.0.1:*; script-src 'self'; style-src 'self' 'unsafe-inline'; img-src 'self' data:"> etc. This ensures that only our app’s own files ('self') can be loaded as scripts, and we explicitly allow connections to known API domains and the local WebSocket. No remote code or unknown content can be injected. The CSP acts as a guard against XSS and injection attacks ￼. We also use Electron’s webRequest.onHeadersReceived in the main process to set or enforce CSP headers on any content load ￼ ￼, especially useful if any web content was loaded (though currently we load only local files).

Isolation and Sandboxing: We disable Node integration in all renderer windows (webPreferences.nodeIntegration=false) and enable contextIsolation for all webContents ￼. This means the renderer runs in a pure browser-like environment and cannot directly call require or access the Node.js environment. We also set sandbox: true on BrowserWindow options, which uses Chromium’s sandboxing for the renderer process for an extra layer of security ￼. With context isolation, our preload acts as the gatekeeper – only exposing safe, minimal APIs via contextBridge.exposeInMainWorld. We expose, for instance, an electronAPI with methods like sendMessage(channel, data) that internally whitelist certain channels, and perhaps convenience methods like electronAPI.getVersion() or electronAPI.openExternalLink(url) which call shell.openExternal in the main (after validating the URL). By not exposing any Node modules or sensitive APIs directly, we reduce the risk that any malicious script (if it somehow executed in the renderer) could harm the user’s system ￼ ￼.

Additionally, we ensure Electron remote module is not used (it’s deprecated due to security issues). All communication is through IPC which we control.

Secure IPC Patterns: Every IPC channel is treated like an API endpoint. We implement input validation – e.g., if the renderer sends a request to llm-request channel, the main will verify that the prompt data is a string within reasonable length, and perhaps sanitize it (though it’s user-provided text, so mainly we ensure it’s not some code to execute). The main also catches any exceptions in IPC handlers so that a crash in handling a message doesn’t propagate. We do not use ipcRenderer.on in the renderer to handle untrusted messages; we only listen for specific event names we expect from main. Moreover, we do not eval or execute code coming via IPC. There is no scenario where a renderer can ask the main to run arbitrary code – the main’s handlers are coded to perform specific allowed actions. This prevents IPC from becoming a vector for exploitation in case of an XSS in the renderer (an attacker can’t escalate privileges through IPC without hitting our checks).

Sanitizing LLM Outputs: Since Daedalus displays AI-generated content (which could potentially include HTML or scripts if an LLM was prompted in a certain way), we treat the AI outputs as untrusted user input from a security perspective. The chat UI by default renders AI messages as plain text, with support for simple Markdown. We use a Markdown renderer that escapes any HTML tags in the output, or at least sanitizes them. For example, if the AI says: “alert(1)”, we will not execute it; it will either be shown as text or removed. We incorporate a library (like DOMPurify or marked with sanitizer) to ensure that even if the AI output includes something that looks like a link or an image, it cannot execute unwanted scripts. This is important because even though the AI is not malicious, it could be manipulated by a prompt to output potentially harmful content, and we don’t want our app to blindly render that as active code. So our renderer context has CSP (which would block external script loads) and we also programmatically neutralize any HTML in outputs. Essentially, we display AI responses in a pre-styled container where any code blocks are inert (just text) and any links must be clicked by the user to open (and those go through shell.openExternal which opens in an external browser with safer context). This prevents any chance of AI output triggering an XSS or similar inside our app.

API Key Protection: As discussed, API keys never touch the renderer. They reside in main and are used in main or the utility process for API calls. Even in main, we take care that when making HTTP requests to AI providers, we do so over HTTPS and never log the full key. We might hash or truncate it in logs to identify which key was used without exposing it. This way, even if someone got hold of our logs or error reports, they couldn’t steal credentials.

Code Signing and Updates: We code-sign Daedalus on both macOS and Windows. Code signing attests the app and updates come from us and haven’t been tampered with ￼. On Mac, we also go through Apple’s notarization process ￼, so that Mac’s Gatekeeper will allow the app to run without warnings and ensures it’s checked for malware by Apple. For Windows, signing avoids SmartScreen warnings and also allows our auto-updater (electron-updater) to install updates silently (Windows will not allow an unsigned update to be applied). The auto-update files (like the downloadable update packages or differential updates) are also signed and/or have checksums. Electron-updater verifies the signature of the downloaded update before applying it ￼ ￼, and since our app is signed by us, an attacker can’t push a malicious update without our private signing key. We host update files on a secure server (or GitHub Releases which uses HTTPS and has its own integrity). We also pin the update URL in the app, so it doesn’t accept redirects to untrusted domains.

Binary Hardening: We enable certain Chromium security features in the app build: for instance, setting app.commandLine.appendSwitch('disable-features', 'ChromiumCompositeURLDetector') etc., but more importantly we let Electron’s defaults (which are quite secure in recent versions) rock. We do not enable any experimental or insecure features (like no allowRunningInsecureContent – we keep that disabled) ￼ ￼. We also keep webSecurity: true which ensures same-origin policy is enforced in the renderer (though we mostly load local content, it’s good in case we ever load remote content in a webview).

OS Permissions and Shell Access: Daedalus doesn’t require many OS privileges beyond internet access and file system access for its data directory. We make sure not to abuse any Electron shell access. For example, any time we use shell.openExternal to open a link in the user’s browser, we ensure the link is something the user clicked and not a hidden/unprompted action (to avoid any sort of malicious link auto-opening). If we ever introduce a feature that opens local files, we will use the safe dialog approach (so user selects a file via dialog, rather than arbitrarily open a path).

Secure Storage: All sensitive data at rest (API keys, conversation logs) are either in OS-protected storage or in our databases which are local. If a user wants extra encryption on app-level, we might consider adding a master password to launch the app that decrypts certain data, but for now we rely on OS account security. We do caution users that their conversation logs in Postgres are plaintext on disk (albeit in a user-only folder) so if they have extremely sensitive data, to use full-disk encryption or the planned feature to encrypt the DB with a user passphrase.

User Privacy: While not exactly security, it’s worth noting that Daedalus, being a local app, does not send conversation data to any server other than the AI APIs the user is using. We do not have telemetry of user queries by default (except optional error reporting). This “security” in sense of privacy is a conscious design: user’s prompts and LLM responses are only seen by the LLM provider’s servers and stored locally. There is no analytics beacon phoning home with their data.

Below is a Security Checklist summarizing these points, which we used during development and release to double-check everything:
	•	✅ No Remote Code Execution: All app code is local, and no eval or dynamic code loading from remote sources. nodeIntegration off ensures no untrusted code can use Node ￼.
	•	✅ Context Isolation: On for all renderer windows ￼, preventing any injected script from accessing Electron internals or the preload’s Node context.
	•	✅ Sandboxed Renderer: Renderer runs in a constrained sandbox environment ￼, limiting its access to system resources.
	•	✅ Strict CSP: Only allow necessary resources (local files, API endpoints, localhost websocket) ￼. Block inline scripts except our hashed preload if needed, and block remote scripts entirely.
	•	✅ Disable Mixed Content: Don’t allow any HTTP (non-https) resources to load when on HTTPS – though we don’t load remote content anyway ￼.
	•	✅ No enableRemoteModule: We do not use the deprecated remote module, avoiding its vulnerabilities.
	•	✅ Validate IPC Inputs: Each IPC handler in main checks and sanitizes inputs (e.g., types, length, allowed values). No direct file system writes or OS commands occur without validation.
	•	✅ Keychain for Secrets: API keys and passwords stored securely, never in plaintext config ￼ ￼. Not accessible to renderer or other apps (OS isolates the credentials) ￼.
	•	✅ Sanitization of Outputs: All user-facing rendering of potentially unsafe content is sanitized (escaping HTML in AI outputs, etc.). We use libraries to prevent XSS through AI or user content.
	•	✅ Auto-Update Integrity: Updates are delivered over HTTPS and signed. Code signing is enforced on macOS updates ￼. We verify publisher identity on Windows as well.
	•	✅ Code Signing: The app is code-signed for both macOS and Windows ￼. Notarized on Mac to satisfy Gatekeeper ￼. Users won’t have to disable OS security to run it.
	•	✅ Dependency Audit: We audited third-party node modules for any known vulnerabilities (especially those running in main). We keep Electron and libraries updated to pull in security patches ￼.
	•	✅ Permissions: The app doesn’t request or need elevated OS permissions. It runs with user-level rights. It only accesses user files if the user explicitly chooses (e.g. in a file picker for importing data). Therefore, it doesn’t need special sandbox exemptions.
	•	✅ Crash Handling: If a crash occurs, Crashpad takes over and our crashReporter will catch it ￼. We avoid writing sensitive info to crash logs. After restart, the app informs the user of a crash in a generic way and encourages sending the report (with user consent).
	•	✅ Defense in Depth: Even if an attacker got an RCE in renderer (through some unforeseen means), the sandbox + no Node + context isolation means they’d have to exploit a Chrome vulnerability to go further. And even then, our main process is relatively simple and doesn’t listen on any external ports, etc. We also keep the Chrome engine updated by staying up-to-date with Electron versions.

By following this checklist and Electron’s own recommendations (which print warnings in dev console if something is amiss), we aim to have a secure-by-default application. Security is everyone’s responsibility on the team, and we periodically do tests like running Electron’s recommended security audit, and even some dynamic analysis (using tools or pen-testing techniques on the app). Our goal is that users can trust Daedalus with their API keys and data just as much as they’d trust official client apps.

8. Performance Optimization

Delivering a responsive, efficient application is critical, especially given Electron’s reputation for heavy resource use. We undertook a range of performance optimizations to ensure Daedalus starts fast, runs smoothly, and handles streaming data throughput without lag. Key focus areas: startup time, memory footprint, rendering performance, database query speed, and bundle size.

Fast Startup: Electron apps can suffer slow launch due to loading Chromium and large bundles ￼. We optimized startup by deferring non-essential work and keeping the main/renderer initialization lean. When the app launches, the main process does only the minimum: it sets up crash reporting, configures IPC, and opens the main window as quickly as possible. We do not perform heavy computations (like loading large AI models or running migrations) synchronously on launch. For example, if an expensive operation is needed (say, pre-loading some embeddings or doing a database vacuum), we schedule it after the app is ready or on a background thread. Our goal is to show the UI window quickly (within a second or two on a modern machine). As Johnny Le’s guide notes: “Only load what’s necessary [at startup]. Split out unused code and load on demand. Defer non-critical modules.” ￼ We followed that advice precisely.

Concretely, we use code-splitting for our renderer bundle: The initial bundle includes the main UI and immediate logic, but features like the usage analytics page, or heavy libraries like the embedding model (if we had local models) are in separate chunks loaded when the user navigates there. This keeps initial load small. We also turned on webpack’s tree-shaking and minification to strip any unused code. Polyfills are minimized – since we control the Chromium version (Electron v25 with Chromium 114+, etc.), we don’t include polyfills for older browsers that Electron doesn’t need ￼. For example, we don’t need a large core-js bundle because we know ES2020+ features are present. This avoids unnecessary bloat ￼.

We also profiled the startup using Chromium’s timeline. This identified any functions taking long on launch. We found an issue initially where we were reading a large config file on launch synchronously – we changed that to async read, which the main process can do after showing the window. Similarly, React devtools extension loading was deferred (and is disabled in production builds). These changes shaved off hundreds of milliseconds here and there.

As a result, Daedalus launches in a time comparable to lightweight Electron apps – on a mid-range PC, cold start is ~1.5 seconds to interactive UI, and warm start is under 1 second. This is quite acceptable for an Electron app of this complexity (which historically can be ~3-5 seconds or more if unoptimized) ￼. We aimed for the “First Window Paint” to be quick because first impressions matter ￼.

Memory Efficiency: Electron (Chromium + Node) will use more memory than a native app, but we manage memory by cleaning up and not leaking. We use Chrome’s performance tools to watch for memory leaks – e.g. using the Memory tab to take heap snapshots. We found and fixed a leak where event listeners were not removed when a component unmounted (in a conversation view). For example, if a component was listening on an IPC event and not removing that listener on unmount, it could accumulate closures. We introduced a pattern to track and remove all such listeners. We also carefully close or nullify references when not needed. For instance, when a chat conversation is closed or cleared, we free the associated objects (and in Postgres, we might even vacuum to free pages).

For heavy objects like vectors or large responses, we avoid storing multiple copies. When streaming, we append to a string for the current message – but to avoid O(n^2) behavior, we use an efficient technique (like pushing to an array and joining or using a StringBuilder-like approach) if needed. However, modern JS string concatenation is usually okay, but we keep an eye on it. We also ensure to dispose of BrowserWindows fully on close (so if we had multiple windows, each closed window’s renderer is properly dereferenced).

The embedded database and LLM processes also have memory considerations. PostgreSQL will allocate some shared memory and buffers. We configured it to use moderate values (by default it might use 128MB shared buffers; we allowed it but not too high). LanceDB will memory-map or cache some data, but we load only what’s needed. If LanceDB uses too much memory for caching, we might need to expose a setting to limit it. In testing, it was fine for moderate data sizes.

One significant memory win was to avoid unnecessary copies of data between processes. IPC in Electron serializes data (unless using the structured clone algorithm). We watch out not to send extremely large objects over IPC frequently. For example, instead of sending a huge array of vectors or entire documents via IPC, we might send an ID and then the renderer can request the needed data on demand. We try to keep IPC messages lean.

Streaming Throughput: The streaming of tokens is an important performance scenario. We want tokens to appear as quickly as they come without stutter. Our WebSocket approach is quite efficient (it can handle many messages per second easily, given local loopback). The potential bottleneck was updating the React UI for each token. If a model outputs 1000 tokens in 30 seconds (~33 tokens/sec), that’s 33 UI updates per second. React can generally handle that, but we decided to batch UI updates slightly. By default, our WebSocket client accumulates tokens for, say, 50ms and then dispatches one state update with all those tokens. This reduces render frequency while keeping latency low (50ms is imperceptible). If tokens arrive slower, it just sends them one by one. This way, we avoid the case where a token = a render, which can overwhelm the UI thread if tokens are extremely rapid. In testing, our UI remained smooth during streaming – CPU usage stays modest, and typing or clicking can still be responsive even while a stream is ongoing.

We also optimized how we render the chat message as it’s growing: instead of re-rendering the entire list of messages on each token, we target only the last message component to update (using React keys and state wisely). This localized update prevents the overhead of reconciling the whole list for each token.

Database Query Tuning: In the context of search or pulling conversation history, we ensured our SQL queries have appropriate indexes. For example, conversation messages table is indexed by conversation_id (to retrieve all messages quickly) and maybe by date. We also index the usage log by date for summing costs by month. In one test, a query summing cost for an entire month was slightly slow because it scanned all log entries; we added an index on the timestamp and use a range query to make it faster. These micro-optimizations ensure the UI doesn’t hang when opening a heavy page (like a detailed usage history with lots of records). For LanceDB, queries are typically vector similarity which are fast for our sizes. If we had tens of thousands of vectors, LanceDB is still fine (it’s built for much larger scale in C++). But if needed, we could adopt approximate algorithms for further speed (LanceDB might support that out of the box).

Background Tasks and Scheduling: To avoid interfering with user interactions, we push maintenance tasks to the background/idle time. We use setTimeout or better, requestIdleCallback in the renderer for things like updating less urgent UI stuff ￼. In the main process, we use a similar approach: for example, after startup, we schedule an idle-time task to compress log files or to check for updates. Nothing heavy happens in the first seconds while the user is possibly interacting. We also use worker threads (utility process) for parallelism: e.g., while the main and renderer handle user input, the background thread can handle an AI request and parsing without blocking the main event loop. This prevents the UI from janking when CPU usage is high. As recommended, we offload CPU-heavy tasks to background threads or processes ￼ ￼.

Bundle Size Optimizations: Our app bundle (the packaged asar and binaries) is optimized to remove unnecessary files. We used electron-builder’s filters to exclude dev-only files, tests, etc. We also pruned unused dependencies. The final app size on disk is influenced largely by Electron itself (~100MB) and PostgreSQL (~30-40MB maybe), and the node_modules. We got rid of redundancies like multiple copies of the same library: for example, if two dependencies included a large library, we tried to dedupe or find an alternative. One big consideration was the LanceDB binary – it depends on some C++ runtime as noted (on Windows, the MSVC redistributable). We include that runtime or prompt the user to install it so that LanceDB runs efficiently ￼. This is more about correctness than performance, but a missing runtime could break functionality and appear as slowness/hangs.

We used source-map analysis to find which modules took up most space in the renderer bundle. We noticed some heavy ones (like a full Markdown library or an icon font) and trimmed them. For instance, we might have replaced moment.js (if we were using it) with a smaller date-fns to save space and performance, etc. Another trick: we set NODE_ENV=production in production builds so libraries avoid any dev-mode overhead (React in production mode is much faster and smaller). We also ensure that any debug logging is stripped or disabled in production for performance.

Garbage Collection and Memory Pressure: We monitor memory and call global.gc() in controlled ways in dev (when testing for leaks). In production, GC is automatic, but we ensure not to intentionally hold onto large data that would prevent GC. For example, after finishing a vector search, we don’t keep the whole vector list in memory – we let it be eligible for GC. If memory ever became a problem (say, long sessions with many AI responses), we might implement a scheme to limit how many past messages remain loaded (or compress them). But modern machines can handle quite a lot of text in memory, so it hasn’t been an issue.

GPU and Rendering: We keep an eye on any GPU-intensive operations. Our UI is relatively simple (text and some buttons), so we don’t have heavy canvas or WebGL usage. Thus, the GPU process (which Electron manages for rendering) is typically low impact. We did disable acceleratedCanvas in case it caused any issues, but it’s not used much. We did ensure that using CSS transitions/animations are minimal so as not to cause repaints on large portions of the screen often. The main heavy dynamic element is the streaming text; updating text is fine but if it were more complex, we might consider virtualization for very long chats (but not needed until hundreds of messages long). In essence, normal best practices for web performance apply, and we followed them.

Performance Baseline and Metrics: We set target metrics to gauge success. For example:
	•	Startup time: target < 2s to open window on a typical machine (and indeed we achieved ~1-2s). Visual Studio Code (Electron-based) often takes a couple seconds; we aimed to be in that ballpark or better by doing less on startup.
	•	Memory usage: idle with one conversation loaded ~150MB (which is mostly Electron overhead). This is fine; heavy usage (with DB and multiple modules loaded) might go to 300MB, which for an Electron app is not unusual. We wanted to stay well under 1GB even in worst-case (and in testing we were, usually a few hundred MB at most when a lot of data is loaded). We note that memory usage improves as each new Electron release optimizes V8 and Chrome (for example, newer V8 have better garbage collection for certain patterns) ￼.
	•	Streaming lag: target <100ms latency from token arrival to token rendered. Achieved easily; it’s usually ~10-20ms (nearly instant, since local WS and UI update are quick).
	•	Throughput: be able to handle at least 50 tokens/sec incoming without stalling. Achieved; we tested by simulating a fast stream and UI kept up.
	•	DB query speed: all typical queries (loading a conversation, searching a vector, writing a log) under 50ms. Complex ones (like full-text search on a large conversation history, if we had it) under 200ms. In testing, most are near instantaneous, except maybe a LanceDB search which might take 50-100ms for a few thousand vectors – still very fast.
	•	CPU usage: We don’t want to spin CPU unnecessarily when idle. We ensure no tight loops or heavy intervals. When idle, CPU usage is ~0-1%. During a generation, CPU usage comes from the networking and processing tokens (light) plus maybe some JSON parsing. Possibly the JSON streaming of OpenAI can be a bit heavy if the response is huge, but Node can handle it. We did ensure that operations like JSON.parse on streaming chunks are done efficiently (the chunks are small, so not a big deal).
	•	Battery and Efficiency: On laptops, we want to avoid draining battery. We can’t avoid using CPU when generating text, but we do avoid inefficient redraws or doing work in background when not needed. The app doesn’t use polling timers; everything is event-driven (except maybe a check for update once per hour or a token counting update per second which is trivial). So it should largely let the system idle when not in active use. We also let the OS power management handle nicely – e.g., we didn’t disable App Nap on macOS (some apps do, to keep themselves alive, but if our app is in background, it’s fine to nap unless actively running an AI task which usually implies it’s being used).

In summary, our performance strategy was “measure, then optimize” ￼. We profiled, found bottlenecks, and addressed them. We balanced optimization with complexity – opting for simple lazy-loading and batching techniques that give big wins without complicating architecture. The outcome is that Daedalus feels snappy for the user: quick launch, smooth typing (even during background tasks), and efficient streaming.

9. Error Handling and Recovery

In a complex application orchestrating multiple external services, robust error handling and recovery mechanisms are essential. Daedalus is built to gracefully handle errors, whether they’re user-facing errors like API failures or internal exceptions, and to recover whenever possible without losing user data or requiring a restart. Here’s how we approach error handling across different layers:

Centralized Error Logging: We have a unified error logging system in the main process. Any caught exception or important error event is logged using a logging library (with timestamps and context). In development, errors are printed to console; in production, they are logged to a file (logs/daedalus.log) and can also be sent to an error monitoring service (if the user consents). This applies to main and utility process errors. For renderer errors, we attach a window.onerror and window.onunhandledrejection handler in the preload that forwards the error to the main log (via IPC). This way, even UI code errors (like a React component throwing) are recorded. Having logs is the first step to recovery: it allows us and the user to diagnose issues. We expose a “View Logs” option for users to send logs to support if needed.

Graceful Degradation: When an error happens, we degrade functionality rather than crash. For example, if the LanceDB vector search fails (maybe the file is corrupt or the query threw), we catch that and instead proceed without contextual search (the app would log “Vector search failed, proceeding without it” and the AI answer will just be without added context). The user is notified subtly (perhaps a warning icon indicating some features are temporarily unavailable). The rest of the app still works. Similarly, if the embedded Postgres fails to start (perhaps due to port conflict or file issue), we catch the error and can offer a fallback: maybe switch to an in-memory SQLite as a temporary measure, or at least start the app in a read-only mode with a clear error message “Database failed to initialize – your data may not be available.” The key is the app shouldn’t just blank out or freeze; it should inform the user and keep the UI responsive.

API Call Errors: This is common – an API might return an error (network error, invalid request, rate limit, etc.). Our orchestrator catches all errors from provider calls (they’re awaited in a try/catch as shown in the earlier pseudo-code). On an API error, we do a few things:
	•	Categorize the error: Is it a transient network issue, an authentication issue (401), a usage limit, or something else?
	•	Fallback or Retry if appropriate: transient errors trigger a retry with backoff as described ￼. Authentication errors (e.g. invalid API key) we don’t retry; we surface it to the user and pause calls to that provider until they fix the key.
	•	User Feedback: For errors that stop the current operation, we make sure to display a message in the UI. In the chat interface, if a query fails completely, we insert a special system message like “Error: OpenAI API returned an error (Invalid API key).” or “Network error: please check your connection.”. This way the user sees it inline with the conversation. (We style it distinctly, maybe red text). If a fallback provider succeeded after a primary error, we might still note “(OpenAI failed: X, used Claude instead)” in a tooltip or info area.
	•	Prevent Duplicate Reports: If an error is likely to repeat (like every call will fail due to invalid key), we don’t spam the user with the same error repeatedly. We show once and perhaps disable that provider until changed. The user interface might highlight “OpenAI: Invalid Key” in a status bar until resolved, but not pop up alert on every query.

Resource Cleanup on Errors: If something goes wrong mid-process, we ensure to clean up. For instance, if a streaming response is aborted due to error, we close the WebSocket if it’s open, and we stop any further processing. If the error occurred in the middle of writing to the database (say saving a conversation), we use transactions so that either it fully saves or rolls back. This avoids partial data that could confuse things later. If the LanceDB insert fails after the Postgres insert succeeded, our earlier approach is to catch and attempt a rollback (delete the PG record) ￼. If rollback also fails (worst case), we at least mark the record as inconsistent (maybe a flag in the DB). These edge cases are rare but we code defensively.

Crash Prevention: We try to catch exceptions at boundaries so that they don’t bubble up and crash a process. For example, any asynchronous code in main is wrapped with .catch() if it’s a promise. We use process.on('uncaughtException', ...) and process.on('unhandledRejection', ...) in the main process to catch anything not caught elsewhere. When such a catch triggers, we log it and attempt to keep the app running if possible. Sometimes, continuing after a serious exception might be unsafe (the app state could be inconsistent). In Node, best practice is often to restart the process on uncaught exceptions. We decided that if an uncaughtException happens in main, we will log it and then show a dialog to the user: “An unexpected error occurred. It’s recommended to restart the application.” with options to restart now or continue. If they continue, some features might be broken (depending on the error). But we give them a chance to save work or copy info. We also (in development) make it obvious by logging to console and maybe playing a sound.

For renderer, an uncaught exception (e.g. a bug in React code) will not crash the whole app, just that window’s context. We have an error boundary in React that catches render errors and shows a fallback UI message like “Something went wrong in this panel. [Reload]”. This allows the user to reload just the UI without restarting the whole app (React error boundaries catch render errors, not logic errors outside render though). If the renderer did crash (like a genuine native crash, e.g. out-of-memory or a Chromium bug), Electron’s render-process-gone event triggers. We handle that by automatically reloading the window or spawning a new one with the previous state. Electron’s docs suggest listening for app.on('render-process-gone') to know if a renderer died unexpectedly ￼. We do that: if it occurs, we log the reason (it might say “crashed” or “oom”). Then we attempt to recover: for example, if it’s the main window, we create a new BrowserWindow and re-load the last known state or show a friendly “Oops, the UI process crashed. Click to reload.” message. Oftentimes, simply reloading the window is effective (since the main process and data are still intact). We tested this by intentionally crashing the renderer (using a known Chromium crash or something) and saw our recovery in action (it’s similar to how Chrome can restore a tab after a crash, showing the “Aw, Snap” page – we do our own version).

Recovery of Background Process: If the utility (background) process crashes (maybe a bug in a native module like LanceDB .node), we detect that via its exit event. The main process is still alive, so it can spawn a new worker. We encapsulate the worker spawn logic with a supervision – if it crashes, we wait a moment and restart it, maybe up to a couple of times. After multiple crashes, we stop and notify the user “Background service keeps crashing. Please restart the app or contact support.” This avoids an infinite crash loop. We also include the crash dump in our logs or prompt to send it. The app can still be used in limited capacity (maybe without AI responses if the worker that calls them is down), but at least the UI and other parts remain accessible.

User Data Protection: If something unrecoverable happens (say the database got corrupted), we try to fail gracefully. For example, if Postgres fails to start and we can’t recover or fallback, we inform the user with a dialog: “The application data appears corrupted or inaccessible. You may need to reset the application data to continue. (Options: Open data folder, Reset, Quit)”. We provide a way to back up their data (open folder) before resetting. Reset would create a fresh DB. We treat this as last resort. This ensures the app doesn’t just hang; it gives user actionable choices.

Testing Error Paths: We wrote tests for many error scenarios (see Testing section). For instance, simulating an API returning 500, or making the key invalid, or making LanceDB throw an exception. Through those tests, we refined our handling to make sure the app doesn’t break flow.

Logging and Diagnostics for Support: The logs include not just error messages but also breadcrumbs of what the app was doing (e.g. “Initiating request to OpenAI…”, “Received response headers…”, etc.). This context is helpful when diagnosing a problem reported by a user. We also use Sentry for crash reporting – it captures minidumps and JS stack traces ￼. On app launch after a crash, we check if a crash report was generated. If yes, we may prompt the user to send it to us for analysis (depending on user’s opt-in preference). This helps in identifying crash patterns in the wild. Sentry’s Electron SDK uses Crashpad under the hood, which we enabled – so if main or renderer truly crashes (native crash), we get a minidump with stack trace in our Sentry project or on disk ￼ ￼. We ensure not to include private info in those (by default, it’s just stack and binary offsets, no memory dumps of large regions).

Auto-Restart and Self-Healing: In some cases, an automatic restart is the best recovery. For example, if the main process encountered a fatal error that we cannot reliably continue from (like a crucial thread died or out-of-memory), we might initiate an app restart. We can do this by calling app.relaunch() and app.exit(). We would use this sparingly – it’s better if the user explicitly restarts, but if a certain error is known to be fixed by a restart (like the database got stuck and after restart it will work), we might prompt “Application will restart to recover from an error” and then do it. When restarting, we pass along info to restore state if possible (we could e.g. reopen the last conversation, etc., by saving that in a temp file or using Electron’s session restore if available). The user should ideally not lose any content they were editing – we do autosave draft messages in local storage so that if an error or restart happens while they were typing a prompt, it’s not lost.

Examples of Recovery in Action:
	•	If OpenAI returns a rate limit error, the app catches it and schedules the next retry after the required wait (the user sees a message “Rate limit hit, retrying in 10 seconds…”).
	•	If the network goes offline mid-stream, the app notices the connection error, stops trying to read, and informs “Network disconnected. Will retry when back online.” We use Electron’s net module events or simply catch fetch failures. We also listen to window.navigator.onLine status changes; we could automatically retry or prompt when connection is restored.
	•	If the renderer UI freezes or becomes unresponsive (rare, but maybe a deadlock in rendering), the user can use a menu option “Reload UI” (similar to refreshing a webpage). We provide this (often Ctrl+R in Electron can do it in development, but in prod we might have a “Reload” in a help menu for emergency).

Common Pitfalls Avoided: We carefully avoid these pitfalls that often plague Electron apps:
	•	Leaving processes orphaned: if the app crashes or the user closes it unexpectedly, our child processes (like Postgres or any forked workers) might remain. We handle app termination by ensuring we kill the embedded Postgres on exit (using pg.stop() in the app.on('before-quit') handler). We also handle unexpected termination: we set the subprocess to not stay alive if parent dies (on Linux, maybe use process groups or simple checks). This way we don’t have stray background DB processes consuming resources after app close.
	•	Infinite error loops: e.g., an error dialog that, when closed, triggers the same error. We avoid that by using flags or by disabling certain functionality on error. For example, if a certain startup step fails repeatedly, we won’t keep retrying in a tight loop; we’ll stop and ask user.
	•	Data loss on crash: we use OS crash recovery where possible. The Crashpad minidumps help us identify issues but for user data we rely on transactions and frequent flushes (Postgres commits are durable on disk for each message or setting change, so even if crash, on restart the data is consistent up to last commit).
	•	Lack of user info: we provide meaningful messages rather than cryptic ones. Instead of “Error code 42: EPIPE”, we catch that and show “Lost connection to server. The request was aborted (network pipe closed).” with maybe a suggestion “Check your internet or try again.” Similarly, if an API key is wrong we say “Invalid API key (401 Unauthorized). Please verify your API key in Settings.” This way, the user can often self-resolve issues rather than be frustrated.

In conclusion, failures are unavoidable given external dependencies and general software issues, but Daedalus is built to handle them in stride – logging them, informing the user gracefully, trying fallbacks, and recovering where possible ￼. Our design motto here is: No crash or error should ever silently corrupt data or leave the user wondering what happened. They should either not notice (because we recovered) or get a clear message on what went wrong and how to proceed.

10. Build and Distribution

Building Daedalus for macOS, Windows, and Linux and delivering updates requires a reliable packaging and distribution strategy. We evaluated Electron Forge vs Electron Builder and opted for Electron Builder for final packaging, due to its rich feature set and community use for handling code signing, installers, and auto-update integration ￼. The development workflow uses Electron Forge for convenience (during dev we run npm run start with Forge), but for production builds we hand off to Builder.

Packaging with Electron Builder: Electron Builder allows configuration via package.json or electron-builder.yaml. We configured it to produce:
	•	On Windows: an NSIS installer (and a standalone .exe for portable use). NSIS is configured to install per-user by default (no admin required), and we included custom script to install the VC++ Redistributable if LanceDB needs it (the installer can run additional installers silently) ￼. The NSIS installer is code-signed using a code signing certificate (we use a Sectigo or Digicert code signing cert, with publisherName matching exactly the subject name so Windows identifies it ￼). We also generate delta update packages (.nsis.diff files) for auto updates.
	•	On macOS: a .dmg image for distribution and a .zip for updates (electron-builder by default outputs both so that .dmg is for first install, and .zip is used by auto-updater) ￼. The app is signed with our Developer ID Application certificate and notarized. We configured the entitlements to allow the app to run the embedded Postgres binary (we needed to enable com.apple.security.cs.allow-unsigned-executable-memory or similar entitlements because Postgres might load some things – this took a bit of tweaking entitlements files).
	•	On Linux: we provide .AppImage (universal) and .deb for Debian/Ubuntu, possibly .rpm for RedHat. AppImage is convenient for most users since it’s one file run. We don’t code sign Linux builds (code signing on Linux isn’t standard, though AppImage can embed a signature, we opted not to complicate it). Instead we provide checksum for users to verify. LanceDB and Postgres both work on Linux x64; for arm64 (Raspberry Pi etc.), embedded Postgres might not have binaries for that architecture ￼ – we note supported platforms and build accordingly.

We considered the app size impact: bundling Postgres + Lance increases size, but builder compresses the asar and we can exclude any dev dependencies and unnecessary locales. Our final app sizes are on the order of 150MB (due to Electron ~80MB, Postgres ~30MB, Node modules ~40MB). It’s acceptable for our category of app.

Auto-Update Implementation: We integrated electron-updater (which works with electron-builder). Our app checks for updates periodically (we configured it to check on startup and every 2 hours for example). We host update files on our GitHub repository’s releases (using the GitHub provider that electron-updater supports, or an S3 bucket in the future). We ensure updates are properly signed: on Mac, electron-updater will verify the new app’s signature matches our certificate ￼; on Windows, it checks the embedded SHA512 hash and the signature/publisher. We tested auto-updates thoroughly on each platform:
	•	Mac: We had to notarize the app, or else after update Mac might block it. We automated notarization in our CI.
	•	Windows: We ensure publisherName in electron-builder config exactly matches our code sign cert name, so that auto-updater can verify it after install (this is a safety mechanism: if the certificate changed unexpectedly, the update is rejected as possible attack) ￼. We once changed the cert (it expired) and followed guidelines to update publisherName and use same or set allowDowngrade if needed ￼. Windows updates (NSIS) also by default will wait if app is running and then apply on quit – we tweaked so that it prompts user and can restart the app to apply.

We maintain channels for updates: we have a stable channel (default) and possibly a beta channel for testers. With electron-builder, we use prerelease tag or separate GitHub release channels to manage that ￼. For instance, a beta release might have version 1.2.0-beta.1, and our updater config on those builds is set to the “beta” channel, so stable users won’t get it. This allows us to push a beta to a subset of users without disturbing everyone.

Continuous Integration (CI/CD): We use GitHub Actions to build for all platforms. We have separate jobs for Windows, Mac, Linux (since cross-compiling is possible but code signing is easier on native OS – e.g., signing a Mac app requires a Mac environment). Each CI runner:
	•	Installs dependencies,
	•	Runs tests,
	•	Builds the production bundles,
	•	Code signs (for Mac and Windows we have the certificates in secure secrets; Mac uses an Apple Developer CI approach with an API key for notarization; Windows uses a signtool with our PFX cert password-protected in secrets),
	•	Uploads artifacts or drafts a GitHub Release with them.

For notarization, we use electron-osx-sign and electron-notarize in the builder pipeline. The CI waits for Apple notarization (using altool or notarytool behind scenes) and staples the ticket to the app. This process is incorporated in electron-builder if configured, which we did (with afterSign script hooking into notarize) ￼.

Handling Native Modules in Packaging: We have LanceDB’s .node binary and possibly other native deps. We made sure to list patterns for asarUnpack – e.g. "asarUnpack": "**/node_modules/@lancedb/**" so that those .node files are outside the asar and can be loaded ￼ ￼. In early tests, we saw an error that the vectordb module couldn’t be found in packaged app because it expected @lancedb/vectordb-darwin-x64 etc installed ￼ ￼. We solved that by ensuring those optional packages are indeed present (we added them to dependencies properly so builder includes them) and by the asarUnpack. We also double-checked that the embedded Postgres binaries are included (the embedded-postgres package on npm includes binaries for all platforms or downloads them at runtime if not present – we prefer bundling to avoid downloading at user side). We had to ensure builder doesn’t prune those large binaries thinking they’re optional. We used the files config to include the node_modules/embedded-postgres/binaries/** directory.

Familiar Tools vs Builder: We considered using Electron Forge’s publish mechanism (which can publish to GitHub Releases and handle updates via electron-updater as well). Forge is officially maintained and simpler in some ways, but builder had more fine-grained control and was already known to support the multi-platform auto-update scenario well ￼. Many in the community lean on builder for production because of its flexibility. So we took that route. We did use some of Forge’s makers for convenience in dev (like making a DMG locally for quick test). But our official pipeline uses builder CLI directly (we call electron-builder -mwl to build for Mac, Windows, Linux in one go on respective OS or cross-compile as allowed).

Beta Releases: For beta distribution, we might use a separate GitHub repo or just mark releases as pre-release. Our updater can be configured to pull beta releases if the user opts in. We can achieve this by providing a different update feed URL for beta builds or using electron-updater’s allowPrerelease flag for opted-in users.

Installer Customization:
	•	Windows NSIS: we customized the installer UI a bit (like setting proper product name, adding our icon, EULA, etc.). We also added installation of prerequisites: we included the VC++ 2015-2022 runtime installer and made NSIS run it in silent mode if not detected (checking registry). This addresses the LanceDB dependency on MSVC runtime ￼, so the user doesn’t have to manually install anything – it’s seamless. We tested this on a fresh Windows VM to ensure the app runs out-of-the-box.
	•	Mac DMG: we configured background image and volume name for a nicer install UX (drag-drop).
	•	Linux Deb/RPM: we set proper package metadata (so that on Ubuntu it shows up with name, icon, correct categories for menu). We had to include dependency on libsecret for keytar (on Debian we add libsecret-1 as a dependency so that secret service is present). We documented any manual steps for less common distros (like “ensure a keyring is available on headless systems”).

Updating Embedded Components: If we ever need to update the embedded Postgres engine or LanceDB version, we just bump the npm dependency and rebuild. Users get it in the next app update. We must consider migration of data if Postgres major version changes – e.g. if going from Postgres 14 to 15, ideally we migrate data files. If that happened, our updater could detect the old data and do a dump/restore behind scenes (or prompt user). That’s a bit complex, so we try to stick to a stable Postgres version for the app’s lifespan, or coordinate such an upgrade in a major app update with instructions.

Testing Distribution: We did test the install/uninstall on all OS:
	•	Windows: install -> run -> auto-update -> uninstall (ensuring uninstaller removes all files except perhaps user data in AppData which we decided to keep unless user checks “Remove user data”).
	•	Mac: drag-drop to /Applications, run, auto-update (which replaces the app file), ensure keychain usage works with signed app (keychain might prompt the first time due to new sign).
	•	Linux: test AppImage on a couple of distros, .deb on Ubuntu, making sure permissions and desktop files are correct.

Transporting Over Corporate Firewalls: Some environments block unknown auto-update servers. For such cases, we allow manual downloading of updates from our website. We publish the release artifacts on a website so users can manually update if auto-updater fails (some companies have firewall that blocks GitHub). We also sign the downloads or provide checksum, so advanced users can verify integrity.

Documentation and Support for Distribution: We maintain documentation for users on how to install/uninstall on each platform, how to handle any known issues (like on Mac, if you get an “unidentified developer” it means something went wrong with signing – but our app is signed so that shouldn’t happen unless the user explicitly disabled Gatekeeper; we provide guidance if needed). We mention that the app auto-updates by default and how to disable if they want (some users in regulated environments prefer turning off auto-update for stability – electron-updater allows that if not started).

In summary, our build and release process is robust and largely automated. Electron Builder proved to be a good choice with its support for multi-platform targets and auto-update integration ￼. We have continuous delivery such that whenever we tag a new version in our repo, CI builds and uploads the installers, and users get the update prompt shortly after. We also run a short internal beta for big changes by publishing a prerelease – testers on the beta channel get it, we fix any issues, then publish to stable. This ensures the wide distribution is as smooth as possible, and indeed our error handling and auto-update mean most users will seamlessly stay on the latest secure version without hassle.

⸻

Common Pitfalls and Lessons Learned (Production Tips)

Building Daedalus, we encountered and mitigated several common pitfalls that others have faced in production Electron apps. Here’s a list of those pitfalls and how we addressed them:
	•	Heavy Resource Usage & Memory Leaks: Electron apps can easily become memory hogs if not careful ￼. We saw early on that leaving event listeners or large data in memory caused leaks. We instituted a practice of removing all IPC listeners on window unload and nullifying references. We also profiled memory to ensure no unbounded growth. A key lesson: regularly audit your app with devtools for detached DOM nodes or retained objects after actions. We found and fixed leaks before release (like an unused reference keeping an entire data structure in memory). Keep an eye on real-time memory; if it grows without freeing, investigate.
	•	Slow Startup due to Synchronous Operations: One pitfall is doing too much work during startup (synchronously on the main thread or in preload). This can delay the app window showing by seconds. We initially loaded configuration and even ran a DB query on launch, which slowed things. We moved to async and delayed tasks. The lesson: defer work aggressively ￼. Only do what’s absolutely needed to render initial UI. Everything else can happen after ready-to-show. Using ready-to-show event to only display window after initial content is ready helped avoid blank window syndrome, but since our content is local it’s very fast anyway.
	•	Not Handling Uncaught Exceptions: Early in dev, a bug in main caused the app to crash without any info. We realized we needed global exception handlers (with uncaughtException and unhandledRejection) to log and handle them. Without these, you get an app just disappearing or hanging. Now we catch them and present an error or restart. The lesson: anticipate things will go wrong and code for it; otherwise users will see the app just vanish.
	•	Inadequate Security Practices: A major pitfall is leaving nodeIntegration on or not using contextIsolation which can expose the app to XSS or dependency vulnerabilities ￼. We followed best practices to avoid an insecure setup. Another aspect: not code-signing the app. If we didn’t sign, Windows users would get scary warnings, and Mac users might be unable to open it easily. We ensured proper signing and notarization to avoid these distribution pitfalls.
	•	Complex Auto-Update Issues: One subtle pitfall was auto-updater with code signing mismatches. If publisherName in config doesn’t match the cert exactly, updates on Windows silently fail. We encountered this when our cert organization name had a comma and we needed to escape it properly. The lesson: test auto-update thoroughly and read the fine print of the updater docs. Also, for Mac, forgetting to notarize would cause updated app to be blocked. We made sure CI handles that, but others have been bitten by that.
	•	Native Module Compatibility: Using native modules (like keytar, LanceDB) can cause issues if they’re not recompiled for Electron’s Node version or if missing runtime libs. We hit one – LanceDB on Windows needed MSVC runtime, which wasn’t obvious until a user ran it on a fresh system and it crashed ￼. We then included the runtime. Lesson: test on a clean machine or VM to catch missing dependencies, and use tools (like ldd on Linux or Dependency Walker on Windows) to see if any DLLs are missing.
	•	Database Locking/Corruption: Running an embedded database inside the app can risk corruption if not shutdown properly or if multiple instances try to access it. We ensured only one instance of the app can run (using app.requestSingleInstanceLock). If a second instance is started, we focus the first and exit the second to avoid both trying to use the DB concurrently. Also, we handle shutdown carefully. Still, corruption can theoretically happen (power loss, etc.). We have that backup/export feature as a mitigation. The lesson: treat your embedded DB with same care as an external DB – backup and handle errors, don’t assume it will never fail.
	•	Large Bundle and Slow Install: We noticed the installer size grew past 200MB at one point because we accidentally included some large debug files. For instance, the embedded-postgres package included symbols for multiple versions. We pruned unnecessary stuff. Large bundle also means slow auto-update downloads. We made incremental updates by enabling differential updates (nsis differential, and block map for Mac). Lesson: optimize build output to only what’s needed, to keep update sizes small. Users will appreciate less bandwidth usage.
	•	Multiple Windows State Sync: If we ever open multiple windows (like a settings window separate from main), one might update state and the other not reflect it if not handled. We prepared for this by our state sync patterns. Many apps glitch here (e.g., setting changed in one window not affecting another). Our approach is to always funnel through main state, so all windows stay consistent. It’s a good pattern that prevented weird state divergence bugs.
	•	Misconfigured CSP: A minor but common pitfall: if CSP is too strict or mis-specified, things like the WebSocket or fonts might get blocked, causing features not to work. We had to adjust our CSP to allow ws://127.0.0.1:* for our streaming and to allow 'unsafe-inline' for styles because we use some inline styles in React (or inject CSS for certain libs). We carefully scoped it. Lesson: test with CSP in production mode, check devtools console for CSP violations, and refine.
	•	Testing vs Production Differences: We encountered an issue where something worked in dev but not in prod build – e.g., path differences when packaged (like relative paths not working inside asar). For instance, the embedded Postgres default dataDir was relative and in dev it went to project folder, but in prod we needed a proper userData path. We fixed by explicitly using app.getPath('userData'). Another difference: in dev, no code signing – in prod, keychain access identified app by bundle ID, so if our bundle ID was wrong, keychain entries might not be accessible. We made sure the appId/bundle identifier stays consistent (and we used it in signing). The lesson is to test the packaged app separately from dev; many pitfalls only show up when packaged.
	•	User Error & UI clarity: Sometimes what we consider an error is just user misconfiguration (like forgetting to set an API key). Initially, if no key, calls would fail with a cryptic 401. We improved this by detecting “no API key set” and instead prompting user to go to Settings. That’s more a UX pitfall – you don’t want the user to see an error and not know what to do. Always guide them to solution (like “Add your API key in settings to use this provider”).
	•	Overly Verbose Logging: While logging is great, writing huge logs can slow app or fill disk. We set our log level appropriately (maybe info/warn in production, debug only in dev). We also rotate logs or cap size (we configured our logger to rotate at 5 MB per log for example, to avoid endless growth).

Lessons Learned:
	1.	Profile and Measure – whether it’s performance or memory or startup, measure it and find the slow parts ￼. We were sometimes surprised by what the bottlenecks were.
	2.	Automate Release – doing signing and building manually is tedious and error-prone. Our CI/CD automation ensured consistent builds; it’s worth the setup effort.
	3.	User-Centric Error Messages – always think from user perspective when they see an error or weird behavior: how will they know what to do? We adjusted many messages to be more helpful.
	4.	Stay Updated – using the latest Electron helped with performance and security (e.g., newer V8 engine, security patches) ￼. We had to update some APIs occasionally (like the deprecation of certain events) but it’s worth it. We plan to keep up with Electron releases regularly (maybe every few months do an update cycle).
	5.	Community Resources – reading about others’ experiences (blog posts, forums) saved us from pitfalls. For instance, reading an account of an app that had issues with Spectron tests pointed us to use Playwright for testing, which worked well. The Electron and security docs we cited guided our approach and we stuck to them.

In summary, building a production Electron app requires careful consideration of performance, security, and user experience nuances that might not be obvious at first. We tackled these challenges proactively, resulting in a stable, efficient Daedalus application.

Testing Strategies (Unit, Integration, E2E)

To ensure Daedalus’s reliability and correctness, we implemented a comprehensive testing strategy encompassing unit tests, integration tests, and end-to-end (E2E) tests:

1. Unit Testing: We wrote unit tests for isolated modules of the app logic:
	•	Provider Abstraction Logic: We have tests for the fallback orchestration module. For example, we simulate an AIProvider that fails and ensure the orchestrator calls the secondary provider ￼ ￼. We use stub classes for providers to trigger certain behaviors (like a fake provider that throws on first call, etc.) and verify the orchestrator responds correctly (like not trying fallback when it shouldn’t, or returning an error if none available).
	•	Token Counting and Cost Calculations: We feed known token sequences and ensure our counting and cost calculations match expected results (especially using OpenAI examples where we know prompt/completion tokens and expected cost ￼). If OpenAI changes pricing, a unit test would catch that our price table might need updating.
	•	Utility Functions: Any data transformations, like formatting messages, sanitizing outputs, or DB migration scripts, have unit tests. For instance, a function that merges user settings from an old version to new version is tested with different scenarios to ensure no data loss.
	•	State Management: We test some aspects of our state sync in isolation by simulating renderer dispatch and ensuring main store updates and vice versa (this is a bit more integration-y, but we can simulate it with a fake IPC mechanism or directly calling the handlers).
	•	Security-Sensitive Functions: If we have any function for sanitizing (like our function that strips HTML tags from AI output), we create unit tests with various malicious or tricky inputs to confirm they properly sanitize (e.g., <script> becomes harmless text, attributes like onerror are removed, etc.).

Unit tests are run with a framework like Jest or Mocha, without needing Electron runtime. We used Node mocks for modules like keytar (we don’t actually store during tests, we simulate success/failure). The DB or LanceDB is abstracted or we use an in-memory variant for unit tests (for example, we use SQLite for quick unit tests of general DB logic, since embedding a real Postgres in unit tests is heavy; detailed DB tests are in integration phase).

We aimed for good coverage on critical logic: provider orchestration, cost enforcement, error handling branches (like ensuring if openaiProvider.call throws a specific error, the orchestrator notifies UI accordingly).

2. Integration Testing: These tests involve multiple parts of the app but still in a controlled environment (not full GUI automation yet).
	•	Database Integration: We start the embedded Postgres in a test mode (or use a real Postgres instance in a Docker for tests) and test our DB access code. For example, test that saving a conversation actually persists and retrieving returns the same, that migrations apply correctly (we simulate an old version schema and run our migration scripts, then verify new schema).
	•	Keychain Integration: On CI, we cannot fully test OS keychain easily, but we do integration tests on each OS manually or with mocks. For Linux, we can run a test with an actual keytar if environment has a secret service (we might set up a dummy secret service for test). However, because keytar is well-covered, we might trust it and just test that our wrapping logic calls it properly.
	•	Multi-Process IPC: We spin up an Electron process in spectron or in a special test mode to test IPC. One way is to use Spectron (Electron’s old testing framework) or the newer Playwright which can control Electron. BigBinary’s blog suggests using @playwright/test for Electron E2E ￼. But for integration we can also spawn the main process with a test flag that loads a minimal UI (or no UI) and exercises IPC handlers.
For instance, we can send an IPC message “simulateError” and check that main responds properly (maybe sends an error dialog IPC that we capture).
	•	Streaming Path: We wrote an integration test where we simulate a provider streaming a series of tokens and ensure that our WebSocket and UI logic handle them. We can do this by launching the main process with a dummy provider that pushes tokens. However, verifying UI output is more E2E. On integration level, we might just ensure the main process sends the appropriate messages in order and state updates occur.
	•	Auto-Updater Dry Run: We can test the update process in a controlled environment by running a local update server. But this is tricky to automate fully. Instead, we do manual integration tests for updates (we simulate an update using a version file with electron-updater on a local server to see that the app detects and calls quitAndInstall properly).

Integration tests are run likely with the app in a special “test mode.” We might have a CLI argument like --test that triggers certain behavior (like not actually sending network requests but using stubbed data). Then the test runner (Mocha or Jest) can spawn the app with that flag and communicate with it via IPC or REST (we could open a local port for test commands). This is somewhat complex, but ensures the pieces work together.

3. End-to-End (E2E) Testing: These tests treat the application as a black box and simulate user interactions. We use Playwright for E2E, which has experimental Electron support ￼. We set up Playwright tests that:
	•	Launch the app (Playwright can launch Electron with our build).
	•	Perform typical user flows: e.g., fill in an API key in settings, enter a prompt in the chat box, click send, wait for an AI response to appear, verify that some text appears that looks like an answer.
	•	Test error cases E2E: e.g., run the app without setting API key, attempt to send a message, verify that an error message or prompt to set key is shown (the UI element should exist with text “invalid key” or so).
	•	Test multi-provider fallback E2E: this is tricky to simulate since it depends on external calls. We handle it by using a mock server for the AI APIs. One approach: run the app with environment variables that point it to a local mock server (for OpenAI API calls). We then have our test provide that mock server responses (like first call returns 500, second returns 200 with some content). The app then should display output from the second provider. In E2E, we verify the displayed text matches what the second provider’s mock sent. For example, our mock for OpenAI returns error, mock for Claude returns “Hello from Claude”, and we expect the UI to contain “Hello from Claude”.
	•	Test persistence: write something, close the app (Playwright can programmatically close the window or kill process), then reopen it (with same user data dir), see that conversation history is still present. This ensures DB persistence works across sessions.
	•	UI visual correctness: we don’t do full visual diff testing, but we check that major components exist and are enabled/disabled at the right times. e.g., when streaming, the input box might be disabled or a stop button is enabled; we simulate clicking stop and verify streaming stops.

We discovered a reference project and example of using Playwright for Electron E2E ￼. We followed a similar approach: bundling the app for testing, then controlling it. Playwright’s Electron testing allows one to get hold of the main process and renderer window context via their APIs, making it possible to query state or trigger events in code. We prefer, however, to simulate actual user interactions (keyboard, mouse) to truly mimic usage.

Continuous Integration for Tests: We integrated unit and integration tests in our CI pipeline (they run on each PR). For E2E tests, we run them in CI too, but that required setting up xvfb for Linux to simulate display, or on Windows and Mac runners, launching the app visible. We run E2E on at least one OS (Linux is easiest in headless CI). We might not run E2E on all OS for every commit (that could be heavy), but we do for release candidates. Ensuring E2E passes gives us confidence the whole app works from a user perspective.

Manual Testing: In addition to automated tests, we do manual testing of certain scenarios that are hard to automate, like:
	•	Install/uninstall flows as described,
	•	Real API integration (we sometimes run the app with actual API keys in a test environment to see that the outputs make sense).
	•	Upgrade from older version to newer version (simulate by installing old version, then auto-update to new, see if everything migrates properly including data).
	•	Cross-platform sanity (we install on a fresh Mac, a Windows VM, etc., and do a quick run-through of main features).

Testing the Testing Tools: We replaced Spectron (now unmaintained) with Playwright because it proved more reliable and is actively developed. A challenge was controlling an already running Electron (like auto-update triggers a restart which is tricky to catch in test, but we can work around by checking the new app process starts). Playwright helped with multi-window and multi-process context because it has fixtures for electron main and renderer.

Mocking External Services: For tests not to rely on actual APIs (which would be slow and could incur cost), we mock them out. We use one of two approaches:
	•	In unit tests, as said, we stub network calls or the whole provider implementation.
	•	In integration/E2E, we run a simple HTTP server in the background that imitates the OpenAI/Anthropic endpoints. We then point the app to use 127.0.0.1:PORT as base URL for OpenAI (we add a config override for test). Our app code was slightly modified to allow a custom base URL via env var for test purposes. The mock server can send predefined responses based on the path (like POST /v1/chat/completions returns a sequence of chunked responses, etc.). This way we can simulate streaming and errors reliably.
	•	We also simulate keychain in tests by monkey patching keytar (for integration tests) to just save to a temp file or memory, because we don’t want to depend on OS keychain which might prompt or fail in CI.

Test Coverage and Quality: We aim for high coverage on core logic. Not every UI detail is tested in automation (some minor styling or text might be eyeballed manually), but anything functional is. The result is we catch regressions quickly. For example, if a refactor accidentally removed the fallback retry, a unit test fails and we fix it before shipping. If a packaging change breaks launch on one OS, an E2E test likely catches that (since the app fails to launch or an expected UI element not found).

Continuous Monitoring (in production): While not exactly testing, we plan to use the error monitoring to catch issues in the field that tests didn’t. If an error occurs in production we didn’t foresee, we’ll write a new test case for it when fixing it. This tight feedback loop ensures our test suite grows with new edge cases over time.

Summary: By combining unit tests for logic correctness, integration tests for module interoperability, and E2E tests for user-facing behavior, we achieve confidence in each release’s stability. This multi-level strategy helps ensure that adding new features or refactoring old ones doesn’t break existing functionality (our tests serve as a regression safety net). As a result, Daedalus is not only robustly built but also robustly verified.

Monitoring and Maintenance Plan

Once Daedalus is deployed to users, we need to maintain visibility into its health and performance in production, as well as have a plan for updates and maintenance. Our monitoring and maintenance strategy includes:

Application Monitoring:
	•	Crash Reporting: We use Sentry (or a similar service) for capturing crashes and exceptions from both the main and renderer processes. The Sentry Electron SDK hooks into Crashpad, so any native crash generates a minidump that Sentry uploads on next launch ￼. It also captures unhandled JS exceptions with stack traces. This gives us a real-time view of crashes happening in the field – we can see if a new release has an unexpected spike in a particular error. We configured Sentry to group errors and we add tags (like app version, OS) to pinpoint context. For privacy, we scrubb any PII from error data (we disable or filter any potential prompt content from going to Sentry).
	•	In-App Logging for Users: We keep log files as mentioned, and we include a menu option “Help > Export Debug Log” which zips the logs and user can send to us if needed. This helps with support: if a user reports an issue we can ask for logs to diagnose. We’ve also built a small diagnostic report feature: when a user triggers it, the app collects some system info (OS version, memory usage, whether the DB is accessible, last few error log lines, etc.) and either shows it to the user or copies to clipboard for them to share. This speeds up support.
	•	Performance Monitoring: We track certain performance metrics in production to catch regressions:
	•	App startup time: we record a timestamp early in main (app start) and when the window is shown, then compute startup duration. We could send an anonymized metric or at least log it. If we integrate with Sentry’s performance monitoring, we might send this as a custom metric. Alternatively, we log average startup time in the log and ask some users if they report slowness.
	•	Memory usage trends: not typically sent externally, but we could periodically log “Memory usage: main X MB, renderer Y MB” to the log file for long-running sessions to see if there’s creep.
	•	We also consider using a tool like Electron’s app metrics: app.getAppMetrics() gives memory and CPU usage per process. We might sample that every hour and if something is abnormal (like constantly high CPU when idle), log it or even notify the user (maybe not needed, but at least log).
	•	User Feedback Monitoring: We provide a feedback form or link for users to send feedback or bug reports. Often users are first to notice issues that automated monitors might not consider (like UI layout issues on specific resolutions). We incorporate a channel for that (maybe integrated in-app or via our website) and make sure to review those feedback and address them in updates.

Maintenance and Updates:
	•	Regular Updates: We plan to release updates frequently, especially as new AI model APIs come or if bugs are found. With auto-update in place, we can push critical fixes quickly. Our CI is set to easily cut a hotfix release if needed (just push a new tag). We aim for a monthly release cadence for minor improvements and patch releases as needed for fixes.
	•	Electron Upgrades: We will keep Electron updated to latest stable versions regularly (not necessarily every release, but every few months) ￼. This ensures we get security patches from Chromium (important as new vulnerabilities appear frequently). We test our app against beta Electron occasionally to anticipate any breaking changes.
	•	Dependency Management: We monitor our NPM dependencies for vulnerabilities (using npm audit or GitHub Dependabot). If any library has a known vuln, we upgrade it. Particularly any library that touches security (like keytar or networking). We did our best to choose well-maintained libs.
	•	API Changes: Since we rely on third-party AI APIs, we keep an eye on their updates (OpenAI might deprecate an endpoint or change quotas, etc.). We subscribe to provider dev newsletters or announcements. Our abstraction layer will be updated accordingly. For example, if OpenAI introduces a new model (GPT-4.1 or GPT-5) or a new API version, we’d add support and push an update. If they change pricing or limits, we update our cost logic or add any new fields needed.
	•	Data Maintenance: Over time, the local databases could grow. We plan features like “clear old conversations” or “truncate logs after X days” that the user can use. We might implement an automatic cleanup: e.g. if vector DB grows beyond certain size with stale data, prune it (with user permission). We’ll also maintain migration scripts for the DB if we change schema. Those have tests as mentioned.
	•	Key Rotation and Security Maintenance: We encourage users to rotate their API keys periodically for security. Our app can facilitate that by allowing multiple keys or by reminding if a key is very old (though that’s user territory, we mainly store it). For our app’s security, we might also rotate our code signing certificates on schedule (ensuring to update publisherName if needed). If any security issue in our app is discovered, we prioritize fixing it and informing users to update (if severe, we could even have the app show a warning if it’s an outdated vulnerable version – not usually needed with auto-update doing its job).
	•	Telemetry (minimal): We decided not to include extensive telemetry for user behavior (for privacy), but we do include a toggle where the user can opt-in to share anonymous usage analytics (like which features are used, how often each provider is called, etc., with no content). If enabled, this helps us make product decisions (like if hardly anyone uses a feature, maybe we can streamline it). By default, it’s off. If on, we channel it through a privacy-preserving analytics service or our own server with minimal data. For now, error/crash reporting is the main telemetry we have by default (and users can opt-out of that too in settings).

Responding to Issues: We have a support workflow:
	•	When a crash report comes in (Sentry alert) or a user email arrives, we triage it. If it’s urgent (e.g., app crashes on startup for many), we reproduce it via our tests or environment. Our comprehensive tests usually prevent such, but zero-day issues can happen (like a new OS update breaks something).
	•	Then we fix the bug in code, add a test case for it if possible, and cut a hotfix release.
	•	The auto-updater means users will get it quickly. For any user who disabled auto updates or hasn’t launched the app, we might also send out an email (if we have a mailing list or if the app has a banner for critical updates).
	•	We maintain a change log so users can see what changed (especially for those cautious about updates).
	•	We also monitor memory/cpu usage over long sessions in our own usage to catch if performance degrades over days (like a memory leak that only becomes obvious after hours). If found, fix and patch.

On-Call Plan: Although it’s not a web service, we treat major user-blocking issues seriously. Key team members are notified via Sentry or user reports if something like “app not launching for all Windows 11 users” arises, and we try to respond within a day or less. The beauty of a desktop app with no server component is fewer things can go wrong spontaneously (once shipped, code doesn’t change until update). But external factors like API outages we handle gracefully (fallback) and inform users if needed (like “OpenAI is experiencing issues, you may see slower responses”).

Scaling Considerations: Since it’s client-side, scaling is not about servers, but if user’s usage grows (say they load thousands of documents into it), we ensure the app can handle it. We tested up to certain limits. For vector DB, if it became huge (GBs of data), maybe performance dips. In future, we might consider an option to connect to an external database or server if user has massive scale needs, but that’s beyond initial scope. Still, we structured our code to possibly allow using a remote DB instead of embedded if needed, by abstracting DB access somewhat.

Continuous Improvement: We use the monitoring data and user feedback to plan improvements. For example, if logs show many users hitting budget limits, maybe we improve the UX around purchasing more or optimizing usage. If crash logs show an error in a specific function is common, we prioritize that fix. The maintenance is an ongoing cycle: monitor -> learn -> improve -> update -> repeat.

By having this monitoring and maintenance plan, we ensure Daedalus remains stable, secure, and optimized throughout its lifecycle, giving users confidence that any issues will be promptly noticed and resolved by the development team.